{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-17T10:12:21.831890Z",
     "start_time": "2025-01-17T10:12:00.961020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "369e9f03d1064e5fa24404bf62d7a1aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a84599a047b4d7480a3b4c91cadf425"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6901a9d302f74305b09799b877f2909b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4361644ad8f4cb6837c34a0b8585dea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dace210c91fe4882a631b0f9c62438b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaibhav/PycharmProjects/llm-visplain/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb322c216352475b9c0b70fb5f99f4d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: encoder.py\n",
      "Content:\n",
      "\"\"\"Byte pair encoding utilities\"\"\"\n",
      "\n",
      "import os\n",
      "import json\n",
      "import regex as re\n",
      "from functools import lru_cache\n",
      "\n",
      "@lru_cache()\n",
      "def bytes_to_unicode():\n",
      "    \"\"\"\n",
      "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
      "    The reversible bpe codes work on unicode strings.\n",
      "    This means \n",
      "\n",
      "\n",
      "File: generate_unconditional_samples.py\n",
      "Content:\n",
      "#!/usr/bin/env python3\n",
      "\n",
      "import fire\n",
      "import json\n",
      "import os\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "import model, sample, encoder\n",
      "\n",
      "def sample_model(\n",
      "    model_name='124M',\n",
      "    seed=None,\n",
      "    nsamples=0,\n",
      "    batch_size=1,\n",
      "    length=None,\n",
      "    temperature=1,\n",
      "    top_k=0,\n",
      "    top_p=1,\n",
      "    models_dir=\n",
      "\n",
      "File: interactive_conditional_samples.py\n",
      "Content:\n",
      "#!/usr/bin/env python3\n",
      "\n",
      "import fire\n",
      "import json\n",
      "import os\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "import model, sample, encoder\n",
      "\n",
      "def interact_model(\n",
      "    model_name='124M',\n",
      "    seed=None,\n",
      "    nsamples=1,\n",
      "    batch_size=1,\n",
      "    length=None,\n",
      "    temperature=1,\n",
      "    top_k=0,\n",
      "    top_p=1,\n",
      "    models_di\n",
      "\n",
      "\n",
      "File: model.py\n",
      "Content:\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from tensorflow.contrib.training import HParams\n",
      "\n",
      "def default_hparams():\n",
      "    return HParams(\n",
      "        n_vocab=0,\n",
      "        n_ctx=1024,\n",
      "        n_embd=768,\n",
      "        n_head=12,\n",
      "        n_layer=12,\n",
      "    )\n",
      "\n",
      "def shape_list(x):\n",
      "    \"\"\"Deal with dynamic shape in tensorf\n",
      "\n",
      "File: sample.py\n",
      "Content:\n",
      "import tensorflow as tf\n",
      "\n",
      "import model\n",
      "\n",
      "def top_k_logits(logits, k):\n",
      "    if k == 0:\n",
      "        # no truncation\n",
      "        return logits\n",
      "\n",
      "    def _top_k():\n",
      "        values, _ = tf.nn.top_k(logits, k=k)\n",
      "        min_values = values[:, -1, tf.newaxis]\n",
      "        return tf.where(\n",
      "            logits < min_values,\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "import tokenizer\n",
    "\n",
    "# Define GitHub API URL and personal access token\n",
    "GITHUB_TOKEN = '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'  # Replace with your token\n",
    "REPO_OWNER = 'rememberlenny'  # Replace with the repository owner (GitHub username or organization)\n",
    "REPO_NAME = 'gpt-2'  # Replace with the name of the repository\n",
    "FILE_PATH = '/src/encoder.py'  # Replace with the path of the file you want to fetch\n",
    "DIRECTORY_PATH = 'src'  # Replace with the path to the directory in the repo\n",
    "model_name = \"microsoft/codebert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Define GitHub API URL to list files in the directory\n",
    "url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/contents/{DIRECTORY_PATH}'\n",
    "\n",
    "# Make the GET request to fetch file contents from the directory\n",
    "response = requests.get(url, headers={'Authorization': f'token {GITHUB_TOKEN}'})\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    files = response.json()\n",
    "\n",
    "    # Iterate over the files in the directory\n",
    "    for file in files:\n",
    "        # If it's a file (not a directory)\n",
    "        if file['type'] == 'file':\n",
    "            file_url = file['download_url']  # GitHub provides a direct URL to download the file\n",
    "            \n",
    "            # Fetch the content of the file\n",
    "            file_response = requests.get(file_url)\n",
    "            \n",
    "            if file_response.status_code == 200:\n",
    "                # If the file is a code file, print its content or process it\n",
    "                print(f\"File: {file['name']}\")\n",
    "                print(\"Content:\")\n",
    "                print(file_response.text[:300])  # Print the first 300 characters of the content\n",
    "                print(\"\\n\")\n",
    "            else:\n",
    "                print(f\"Error fetching file content: {file['name']}\")\n",
    "else:\n",
    "    print(f'Error: {response.status_code}, {response.text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scraping code files from repo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56a69a92f5969a2e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_functions_from_code(code):\n",
    "    \"\"\"\n",
    "    Extract functions from a given code (in Python or similar language)\n",
    "    and return a list of function names and their corresponding code chunks.\n",
    "    \"\"\"\n",
    "    # Regex to match function definitions (Python-style)\n",
    "    function_pattern = r\"def (\\w+)\\((.*?)\\):\\s*(.*?)\\n(?=\\s*def|\\s*$)\"\n",
    "    \n",
    "    functions = []\n",
    "    \n",
    "    # Find all function matches using regex\n",
    "    matches = re.findall(function_pattern, code, re.DOTALL)\n",
    "    \n",
    "    for match in matches:\n",
    "        func_name = match[0]\n",
    "        func_code = match[2].strip()  # Extract function code\n",
    "        functions.append((func_name, func_code))\n",
    "    \n",
    "    return functions\n",
    "\n",
    "def fetch_files_from_directory(repo_owner, repo_name, dir_path=''):\n",
    "    # Define GitHub API URL to list files in the directory\n",
    "    print(\"Debug: Entering function...\")\n",
    "    url = f'https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{dir_path}'\n",
    "    print(f\"API URL: {url}\")  # Print the full URL to check if it's correct\n",
    "\n",
    "    # Make the GET request to fetch file contents from the directory\n",
    "    response = requests.get(url, headers={'Authorization': f'token {GITHUB_TOKEN}'})\n",
    "    \n",
    "    # Debugging the response\n",
    "    print(f\"Response Status Code: {response.status_code}\")\n",
    "    print(f\"Response Content: {response.text}\")  # Print the raw response content for debugging\n",
    "\n",
    "    files_list = []  # Initialize an empty list to hold the files and chunks\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        files = response.json()\n",
    "        if not files:\n",
    "            print(\"No files found in this directory.\")\n",
    "        else:\n",
    "            print(f\"Files found: {len(files)}\")  # Print number of files found\n",
    "            for file in files:\n",
    "                print(f\"File: {file['name']} - Type: {file['type']}\")\n",
    "                if file['type'] == 'file':\n",
    "                    # Fetch file content\n",
    "                    file_url = file['download_url']\n",
    "                    file_response = requests.get(file_url)\n",
    "                    print(f\"Fetching file from URL: {file_url}\")\n",
    "                    if file_response.status_code == 200:\n",
    "                        file_content = file_response.text\n",
    "                        file_name = file['name']\n",
    "                        print(f\"Fetching content from file: {file_name}\")\n",
    "                        # Process the file content for RAG (you can chunk text here if needed)\n",
    "                        function_chunks = extract_functions_from_code(file_content)\n",
    "                        for func_name, func_code in function_chunks:\n",
    "                            files_list.append((file_name, func_name, func_code))\n",
    "                        # for chunk in chunks:\n",
    "                        # files_list.append((file_name, file_content))  # Append filename and chunk to the list\n",
    "                    else:\n",
    "                        print(f\"Error fetching content from {file_name}, Status code: {file_response.status_code}\")\n",
    "                elif file['type'] == 'dir':\n",
    "                    # Recurse into subdirectory and extend the list\n",
    "                    files_list.extend(fetch_files_from_directory(repo_owner, repo_name, file['path']))\n",
    "    else:\n",
    "        print(f'Error: {response.status_code}, {response.text}')\n",
    "\n",
    "    return files_list  # Return the list of files and chunks\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T10:21:06.405780Z",
     "start_time": "2025-01-22T10:21:06.395274Z"
    }
   },
   "id": "bddf501d8af04e43",
   "execution_count": 167
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating Code Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "304f036b8dadfec3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_code_embeddings(code_chunks):\n",
    "    embeddings = []\n",
    "    for file_name, code in code_chunks:\n",
    "        inputs = tokenizer(code, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy().squeeze()\n",
    "            embeddings.append((file_name, embedding))\n",
    "    return embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T10:21:09.980239Z",
     "start_time": "2025-01-22T10:21:09.974052Z"
    }
   },
   "id": "a07521a55becac0d",
   "execution_count": 168
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Entering function...\n",
      "API URL: https://api.github.com/repos/rememberlenny/gpt-2/contents/\n",
      "Response Status Code: 200\n",
      "Response Content: [{\"name\":\".gitattributes\",\"path\":\".gitattributes\",\"sha\":\"7c3a822f3a40affc08926148e8ebecf792d7c302\",\"size\":191,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/.gitattributes?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/.gitattributes\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/7c3a822f3a40affc08926148e8ebecf792d7c302\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/.gitattributes\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/.gitattributes?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/7c3a822f3a40affc08926148e8ebecf792d7c302\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/.gitattributes\"}},{\"name\":\".gitignore\",\"path\":\".gitignore\",\"sha\":\"5b1de5ab8dc2c7d1491de0dc36742d121f078733\",\"size\":33,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/.gitignore?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/.gitignore\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/5b1de5ab8dc2c7d1491de0dc36742d121f078733\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/.gitignore\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/.gitignore?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/5b1de5ab8dc2c7d1491de0dc36742d121f078733\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/.gitignore\"}},{\"name\":\"CONTRIBUTORS.md\",\"path\":\"CONTRIBUTORS.md\",\"sha\":\"eab7132a35ff59b3b3b8b4b64282ecbbfc54bd4e\",\"size\":551,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/CONTRIBUTORS.md?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/CONTRIBUTORS.md\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/eab7132a35ff59b3b3b8b4b64282ecbbfc54bd4e\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/CONTRIBUTORS.md\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/CONTRIBUTORS.md?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/eab7132a35ff59b3b3b8b4b64282ecbbfc54bd4e\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/CONTRIBUTORS.md\"}},{\"name\":\"DEVELOPERS.md\",\"path\":\"DEVELOPERS.md\",\"sha\":\"d23c9d05f529646b788ebe0f63487ba8a6b276ca\",\"size\":2188,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/DEVELOPERS.md?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/DEVELOPERS.md\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/d23c9d05f529646b788ebe0f63487ba8a6b276ca\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/DEVELOPERS.md\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/DEVELOPERS.md?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/d23c9d05f529646b788ebe0f63487ba8a6b276ca\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/DEVELOPERS.md\"}},{\"name\":\"Dockerfile.cpu\",\"path\":\"Dockerfile.cpu\",\"sha\":\"b6e4f949631430acd2fb80fbd55d050d08b48219\",\"size\":279,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/Dockerfile.cpu?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/Dockerfile.cpu\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/b6e4f949631430acd2fb80fbd55d050d08b48219\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/Dockerfile.cpu\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/Dockerfile.cpu?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/b6e4f949631430acd2fb80fbd55d050d08b48219\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/Dockerfile.cpu\"}},{\"name\":\"Dockerfile.gpu\",\"path\":\"Dockerfile.gpu\",\"sha\":\"5ac049aff9b20abc918b4595f77be15cd38cc849\",\"size\":548,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/Dockerfile.gpu?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/Dockerfile.gpu\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/5ac049aff9b20abc918b4595f77be15cd38cc849\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/Dockerfile.gpu\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/Dockerfile.gpu?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/5ac049aff9b20abc918b4595f77be15cd38cc849\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/Dockerfile.gpu\"}},{\"name\":\"LICENSE\",\"path\":\"LICENSE\",\"sha\":\"f56abfef079f3850e93db8bd6ee32582c3e65956\",\"size\":1403,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/LICENSE?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/LICENSE\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/f56abfef079f3850e93db8bd6ee32582c3e65956\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/LICENSE\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/LICENSE?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/f56abfef079f3850e93db8bd6ee32582c3e65956\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/LICENSE\"}},{\"name\":\"README.md\",\"path\":\"README.md\",\"sha\":\"6d998ece1525870cbdd62436914c55866725f137\",\"size\":2827,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/README.md?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/README.md\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/6d998ece1525870cbdd62436914c55866725f137\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/README.md\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/README.md?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/6d998ece1525870cbdd62436914c55866725f137\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/README.md\"}},{\"name\":\"domains.txt\",\"path\":\"domains.txt\",\"sha\":\"04bdac448be602ce638b231a41988a41cfcc1028\",\"size\":14754,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/domains.txt?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/domains.txt\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/04bdac448be602ce638b231a41988a41cfcc1028\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/domains.txt\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/domains.txt?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/04bdac448be602ce638b231a41988a41cfcc1028\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/domains.txt\"}},{\"name\":\"download_model.py\",\"path\":\"download_model.py\",\"sha\":\"54e4bb60c0f80efccb05f59ab7876f515d24c720\",\"size\":1075,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/download_model.py?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/download_model.py\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/54e4bb60c0f80efccb05f59ab7876f515d24c720\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/download_model.py\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/download_model.py?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/54e4bb60c0f80efccb05f59ab7876f515d24c720\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/download_model.py\"}},{\"name\":\"model_card.md\",\"path\":\"model_card.md\",\"sha\":\"38246ee2ae274461b154f5ab246e4892e5a50e95\",\"size\":4989,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/model_card.md?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/model_card.md\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/38246ee2ae274461b154f5ab246e4892e5a50e95\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/model_card.md\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/model_card.md?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/38246ee2ae274461b154f5ab246e4892e5a50e95\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/model_card.md\"}},{\"name\":\"requirements.txt\",\"path\":\"requirements.txt\",\"sha\":\"2cc521d5e129b205b292e278d4d111faed460f75\",\"size\":58,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/requirements.txt?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/requirements.txt\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/2cc521d5e129b205b292e278d4d111faed460f75\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/requirements.txt\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/requirements.txt?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/2cc521d5e129b205b292e278d4d111faed460f75\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/requirements.txt\"}},{\"name\":\"src\",\"path\":\"src\",\"sha\":\"cf8133fd9220fa01ffb1a6eaed2aab224e6b9827\",\"size\":0,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/tree/master/src\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/trees/cf8133fd9220fa01ffb1a6eaed2aab224e6b9827\",\"download_url\":null,\"type\":\"dir\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/trees/cf8133fd9220fa01ffb1a6eaed2aab224e6b9827\",\"html\":\"https://github.com/rememberlenny/gpt-2/tree/master/src\"}}]\n",
      "Files found: 13\n",
      "File: .gitattributes - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/.gitattributes\n",
      "Fetching content from file: .gitattributes\n",
      "File: .gitignore - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/.gitignore\n",
      "Fetching content from file: .gitignore\n",
      "File: CONTRIBUTORS.md - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/CONTRIBUTORS.md\n",
      "Fetching content from file: CONTRIBUTORS.md\n",
      "File: DEVELOPERS.md - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/DEVELOPERS.md\n",
      "Fetching content from file: DEVELOPERS.md\n",
      "File: Dockerfile.cpu - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/Dockerfile.cpu\n",
      "Fetching content from file: Dockerfile.cpu\n",
      "File: Dockerfile.gpu - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/Dockerfile.gpu\n",
      "Fetching content from file: Dockerfile.gpu\n",
      "File: LICENSE - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/LICENSE\n",
      "Fetching content from file: LICENSE\n",
      "File: README.md - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/README.md\n",
      "Fetching content from file: README.md\n",
      "File: domains.txt - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/domains.txt\n",
      "Fetching content from file: domains.txt\n",
      "File: download_model.py - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/download_model.py\n",
      "Fetching content from file: download_model.py\n",
      "File: model_card.md - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/model_card.md\n",
      "Fetching content from file: model_card.md\n",
      "File: requirements.txt - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/requirements.txt\n",
      "Fetching content from file: requirements.txt\n",
      "File: src - Type: dir\n",
      "Debug: Entering function...\n",
      "API URL: https://api.github.com/repos/rememberlenny/gpt-2/contents/src\n",
      "Response Status Code: 200\n",
      "Response Content: [{\"name\":\"encoder.py\",\"path\":\"src/encoder.py\",\"sha\":\"5f52e723cfc27bcfa92b2bcb395115f8c8c01502\",\"size\":4242,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src/encoder.py?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/src/encoder.py\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/5f52e723cfc27bcfa92b2bcb395115f8c8c01502\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/src/encoder.py\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src/encoder.py?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/5f52e723cfc27bcfa92b2bcb395115f8c8c01502\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/src/encoder.py\"}},{\"name\":\"generate_unconditional_samples.py\",\"path\":\"src/generate_unconditional_samples.py\",\"sha\":\"eaf9a63fc1833febdbae4248f2166e1fcc24a05d\",\"size\":2870,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src/generate_unconditional_samples.py?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/src/generate_unconditional_samples.py\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/eaf9a63fc1833febdbae4248f2166e1fcc24a05d\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/src/generate_unconditional_samples.py\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src/generate_unconditional_samples.py?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/eaf9a63fc1833febdbae4248f2166e1fcc24a05d\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/src/generate_unconditional_samples.py\"}},{\"name\":\"interactive_conditional_samples.py\",\"path\":\"src/interactive_conditional_samples.py\",\"sha\":\"8b66000d79260f28189520dc5173a699d651e15d\",\"size\":3412,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src/interactive_conditional_samples.py?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/src/interactive_conditional_samples.py\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/8b66000d79260f28189520dc5173a699d651e15d\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/src/interactive_conditional_samples.py\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src/interactive_conditional_samples.py?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/8b66000d79260f28189520dc5173a699d651e15d\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/src/interactive_conditional_samples.py\"}},{\"name\":\"model.py\",\"path\":\"src/model.py\",\"sha\":\"230b83cc2ac7582b4a22e9db3c2d8916dfe2ea9a\",\"size\":6503,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src/model.py?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/src/model.py\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/230b83cc2ac7582b4a22e9db3c2d8916dfe2ea9a\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/src/model.py\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src/model.py?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/230b83cc2ac7582b4a22e9db3c2d8916dfe2ea9a\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/src/model.py\"}},{\"name\":\"sample.py\",\"path\":\"src/sample.py\",\"sha\":\"c90ed28dc404ff14b4dcd284f747d2682ee3fd65\",\"size\":3166,\"url\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src/sample.py?ref=master\",\"html_url\":\"https://github.com/rememberlenny/gpt-2/blob/master/src/sample.py\",\"git_url\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/c90ed28dc404ff14b4dcd284f747d2682ee3fd65\",\"download_url\":\"https://raw.githubusercontent.com/rememberlenny/gpt-2/master/src/sample.py\",\"type\":\"file\",\"_links\":{\"self\":\"https://api.github.com/repos/rememberlenny/gpt-2/contents/src/sample.py?ref=master\",\"git\":\"https://api.github.com/repos/rememberlenny/gpt-2/git/blobs/c90ed28dc404ff14b4dcd284f747d2682ee3fd65\",\"html\":\"https://github.com/rememberlenny/gpt-2/blob/master/src/sample.py\"}}]\n",
      "Files found: 5\n",
      "File: encoder.py - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/src/encoder.py\n",
      "Fetching content from file: encoder.py\n",
      "File: generate_unconditional_samples.py - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/src/generate_unconditional_samples.py\n",
      "Fetching content from file: generate_unconditional_samples.py\n",
      "File: interactive_conditional_samples.py - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/src/interactive_conditional_samples.py\n",
      "Fetching content from file: interactive_conditional_samples.py\n",
      "File: model.py - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/src/model.py\n",
      "Fetching content from file: model.py\n",
      "File: sample.py - Type: file\n",
      "Fetching file from URL: https://raw.githubusercontent.com/rememberlenny/gpt-2/master/src/sample.py\n",
      "Fetching content from file: sample.py\n"
     ]
    }
   ],
   "source": [
    "files_content = fetch_files_from_directory(REPO_OWNER, REPO_NAME, \"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T10:21:16.972103Z",
     "start_time": "2025-01-22T10:21:11.687279Z"
    }
   },
   "id": "bd49192dbda18344",
   "execution_count": 169
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[170], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_code_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfiles_content\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[168], line 8\u001B[0m, in \u001B[0;36mgenerate_code_embeddings\u001B[0;34m(code_chunks)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_code_embeddings\u001B[39m(code_chunks):\n\u001B[1;32m      7\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m file_name, code \u001B[38;5;129;01min\u001B[39;00m code_chunks:\n\u001B[1;32m      9\u001B[0m         inputs \u001B[38;5;241m=\u001B[39m tokenizer(code, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m)\n\u001B[1;32m     10\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "embeddings = generate_code_embeddings(files_content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T10:21:17.026822Z",
     "start_time": "2025-01-22T10:21:16.975196Z"
    }
   },
   "id": "17c3b3ed5333f104",
   "execution_count": 170
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build kNN Graph using networkx"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cef8ef687d92e0d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def build_knn_graph(embeddings, k=3):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes to the graph (each node is a code file)\n",
    "    for idx, (file_name, embedding) in enumerate(embeddings):\n",
    "        # print(file_name, embedding)\n",
    "        G.add_node(idx, file_name=file_name, embedding=embedding)\n",
    "    \n",
    "    # Add edges based on similarity between file embeddings (Cosine similarity)\n",
    "    for i, (file_name_i, embedding_i) in enumerate(embeddings):\n",
    "        similarities = []\n",
    "        for j, (file_name_j, embedding_j) in enumerate(embeddings):\n",
    "            if i != j:\n",
    "                similarity = np.dot(embedding_i, embedding_j) / (np.linalg.norm(embedding_i) * np.linalg.norm(embedding_j))  # Cosine similarity\n",
    "                similarities.append((j, similarity))\n",
    "        \n",
    "        # Sort by similarity and keep the top K neighbors\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_k_neighbors = similarities[:k]\n",
    "        \n",
    "        for neighbor_idx, similarity in top_k_neighbors:\n",
    "            G.add_edge(i, neighbor_idx, weight=similarity)\n",
    "    \n",
    "    return G"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1011d250b5025104",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "G = build_knn_graph(embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T10:21:18.026127Z",
     "start_time": "2025-01-22T10:21:18.020278Z"
    }
   },
   "id": "6b7c617cb959930e",
   "execution_count": 171
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Querying kNN graph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d87f4813d2725f37"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def query_kag(query, k=3, G=None, embeddings=None, code_chunks=None):\n",
    "    # Generate the embedding for the query using CodeBERT\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        query_embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy().squeeze()\n",
    "    \n",
    "    # Find the node with the highest similarity (query vs. node embeddings)\n",
    "    similarities = []\n",
    "    for node in G.nodes:\n",
    "        node_embedding = G.nodes[node][\"embedding\"]\n",
    "        similarity = np.dot(query_embedding, node_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding))  # Cosine similarity\n",
    "        similarities.append((node, similarity))\n",
    "    \n",
    "    # Sort by similarity and get top k\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_k_nodes = [node for node, _ in similarities[:k]]\n",
    "    \n",
    "    # Debug: Check if top_k_nodes are correct\n",
    "    print(f\"Top {k} nodes: {top_k_nodes}\")\n",
    "    \n",
    "    # Retrieve the corresponding code chunks (files) from the graph nodes\n",
    "    context = []\n",
    "    for node in top_k_nodes:\n",
    "        file_name = G.nodes[node]['file_name']\n",
    "        chunk = code_chunks[node][1]  # Assuming code_chunks[node][1] contains the code snippet\n",
    "        \n",
    "        # Debug: Check the content of each chunk\n",
    "        print(f\"Adding context from file: {file_name}\")\n",
    "        print(f\"Code chunk: {chunk[:100]}...\")  # Print first 100 characters of the chunk for debugging\n",
    "        \n",
    "        context.append(f\"File: {file_name}\\nChunk:\\n{chunk}\\n\")\n",
    "    \n",
    "    # Combine the query with the context for the LLaMA model\n",
    "    full_prompt = f\"Query: {query}\\nContext:\\n\" + \"\\n\".join(context)\n",
    "    \n",
    "    # Debug: Print the full prompt before sending it to LLaMA\n",
    "    print(f\"Full prompt:\\n{full_prompt[:500]}...\")  # Print first 500 characters for debugging\n",
    "    \n",
    "    return full_prompt\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T10:21:18.763999Z",
     "start_time": "2025-01-22T10:21:18.759543Z"
    }
   },
   "id": "141dab4c07a0a45a",
   "execution_count": 172
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "def generate_answer_with_ollama(context):\n",
    "    # Send the prompt to Ollama for LLaMA processing\n",
    "    response = ollama.chat(model=\"llama3\", messages=[{\"role\": \"user\", \"content\": context}])\n",
    "    \n",
    "    # Debug: Check the response structure\n",
    "    print(f\"Response from Ollama: {response}\")\n",
    "    \n",
    "    # Extract the answer\n",
    "    answer = response[\"message\"][\"content\"]\n",
    "    return answer\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T10:21:20.338310Z",
     "start_time": "2025-01-22T10:21:20.334258Z"
    }
   },
   "id": "c64dda8c9b68416c",
   "execution_count": 173
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 nodes: [14, 13, 2, 12, 5, 7, 4, 16, 10, 9]\n",
      "Adding context from file: interactive_conditional_samples.py\n",
      "Code chunk: split_states...\n",
      "Adding context from file: generate_unconditional_samples.py\n",
      "Code chunk: norm...\n",
      "Adding context from file: CONTRIBUTORS.md\n",
      "Code chunk: __init__...\n",
      "Adding context from file: encoder.py\n",
      "Code chunk: gelu...\n",
      "Adding context from file: Dockerfile.gpu\n",
      "Code chunk: decode...\n",
      "Adding context from file: README.md\n",
      "Code chunk: sample_model...\n",
      "Adding context from file: Dockerfile.cpu\n",
      "Code chunk: encode...\n",
      "Adding context from file: sample.py\n",
      "Code chunk: conv1d...\n",
      "Adding context from file: model_card.md\n",
      "Code chunk: shape_list...\n",
      "Adding context from file: download_model.py\n",
      "Code chunk: default_hparams...\n",
      "Full prompt:\n",
      "Query: Please analyze the following code snippets and provide a summary of the repository architecture. Specifically,           I would like a description of the main components, including: \n",
      "        1. Key **classes** and their attributes. \n",
      "        2. Important **functions** and methods within those classes. \n",
      "        3. The overall **design** of the architecture and how the components interact with each other. \n",
      "         \n",
      "        The code snippets represent various files from the repository, and ...\n",
      "Response from Ollama: model='llama3' created_at='2025-01-22T10:24:42.268808Z' done=True done_reason='stop' total_duration=24868887542 load_duration=31982500 prompt_eval_count=242 prompt_eval_duration=1186000000 eval_count=592 eval_duration=23650000000 message=Message(role='assistant', content=\"Based on the provided code snippets, I'll summarize the repository architecture. Please note that this analysis is based on the assumption that these snippets represent various files from the same repository.\\n\\n**Key Classes and their Attributes:**\\n\\n1. `sample.py`: The `conv1d` class represents a convolutional neural network (CNN) model for generating conditional samples.\\n2. `encoder.py`: The `gelu` function appears to be a variant of the GELU (Gaussian Error Linear Unit) activation function used in the encoder module.\\n3. `generate_unconditional_samples.py`: The `norm` function is likely related to normalizing the input data or output samples.\\n\\n**Important Functions and Methods:**\\n\\n1. `split_states` (interactive_conditional_samples.py): This function might be responsible for splitting or partitioning the state space into smaller regions.\\n2. `decode` (Dockerfile.gpu): This function could be involved in decoding or processing the output of the model, possibly for generating samples.\\n3. `encode` (Dockerfile.cpu): This function likely performs encoding or preprocessing on the input data before feeding it to the model.\\n\\n**Overall Design and Component Interaction:**\\n\\nThe repository appears to be focused on generating conditional samples using a CNN-based architecture. The main components are:\\n\\n1. **Model**: Represented by the `conv1d` class in `sample.py`, this is the core component responsible for generating conditional samples.\\n2. **Encoder**: Implemented in `encoder.py`, this module seems to play a crucial role in processing input data and preparing it for the model.\\n3. **Preprocessing**: The `norm` function in `generate_unconditional_samples.py` might be used to normalize the input data or output samples.\\n\\nThe architecture likely involves the following interactions:\\n\\n1. Input data is processed using the encoder (encode) and then fed into the model (conv1d).\\n2. The model generates conditional samples, which are then decoded or processed using the decode function.\\n3. Preprocessing steps might be applied to the input data or output samples using the norm function.\\n\\n**Other Components:**\\n\\n1. **Dockerfiles**: The Dockerfile.gpu and Dockerfile.cpu files suggest that the repository is designed for deployment on GPU-enabled and CPU-only environments, respectively.\\n2. **README.md**: This file likely provides a high-level overview of the project, including information about the model, its components, and how to use it.\\n3. **CONTRIBUTORS.md**: This file lists contributors to the project, which is an important aspect of open-source software development.\\n4. **model_card.md**: The shape_list chunk might represent information about the model's architecture or hyperparameters.\\n\\nIn summary, the repository appears to be a machine learning project focused on generating conditional samples using a CNN-based architecture. The main components are the model (conv1d), encoder (gelu), and preprocessing steps (norm).\", images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"Please analyze the following code snippets and provide a summary of the repository architecture. Specifically,           I would like a description of the main components, including: \n",
    "        1. Key **classes** and their attributes. \n",
    "        2. Important **functions** and methods within those classes. \n",
    "        3. The overall **design** of the architecture and how the components interact with each other. \n",
    "         \n",
    "        The code snippets represent various files from the repository, and you should use them to summarize the codebase's  architecture. \n",
    "         \n",
    "        Here are the code snippets from the repository:\\n\"\"\"\n",
    "\n",
    "context = query_kag(query, k=10, G=G, embeddings=embeddings, code_chunks=files_content)\n",
    "# print(context)\n",
    "answer = generate_answer_with_ollama(context)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T10:24:42.288897Z",
     "start_time": "2025-01-22T10:24:17.269192Z"
    }
   },
   "id": "6e561dd2bc10633c",
   "execution_count": 176
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided code snippets, I'll summarize the repository architecture. Please note that this analysis is based on the assumption that these snippets represent various files from the same repository.\n",
      "\n",
      "**Key Classes and their Attributes:**\n",
      "\n",
      "1. `sample.py`: The `conv1d` class represents a convolutional neural network (CNN) model for generating conditional samples.\n",
      "2. `encoder.py`: The `gelu` function appears to be a variant of the GELU (Gaussian Error Linear Unit) activation function used in the encoder module.\n",
      "3. `generate_unconditional_samples.py`: The `norm` function is likely related to normalizing the input data or output samples.\n",
      "\n",
      "**Important Functions and Methods:**\n",
      "\n",
      "1. `split_states` (interactive_conditional_samples.py): This function might be responsible for splitting or partitioning the state space into smaller regions.\n",
      "2. `decode` (Dockerfile.gpu): This function could be involved in decoding or processing the output of the model, possibly for generating samples.\n",
      "3. `encode` (Dockerfile.cpu): This function likely performs encoding or preprocessing on the input data before feeding it to the model.\n",
      "\n",
      "**Overall Design and Component Interaction:**\n",
      "\n",
      "The repository appears to be focused on generating conditional samples using a CNN-based architecture. The main components are:\n",
      "\n",
      "1. **Model**: Represented by the `conv1d` class in `sample.py`, this is the core component responsible for generating conditional samples.\n",
      "2. **Encoder**: Implemented in `encoder.py`, this module seems to play a crucial role in processing input data and preparing it for the model.\n",
      "3. **Preprocessing**: The `norm` function in `generate_unconditional_samples.py` might be used to normalize the input data or output samples.\n",
      "\n",
      "The architecture likely involves the following interactions:\n",
      "\n",
      "1. Input data is processed using the encoder (encode) and then fed into the model (conv1d).\n",
      "2. The model generates conditional samples, which are then decoded or processed using the decode function.\n",
      "3. Preprocessing steps might be applied to the input data or output samples using the norm function.\n",
      "\n",
      "**Other Components:**\n",
      "\n",
      "1. **Dockerfiles**: The Dockerfile.gpu and Dockerfile.cpu files suggest that the repository is designed for deployment on GPU-enabled and CPU-only environments, respectively.\n",
      "2. **README.md**: This file likely provides a high-level overview of the project, including information about the model, its components, and how to use it.\n",
      "3. **CONTRIBUTORS.md**: This file lists contributors to the project, which is an important aspect of open-source software development.\n",
      "4. **model_card.md**: The shape_list chunk might represent information about the model's architecture or hyperparameters.\n",
      "\n",
      "In summary, the repository appears to be a machine learning project focused on generating conditional samples using a CNN-based architecture. The main components are the model (conv1d), encoder (gelu), and preprocessing steps (norm).\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T10:24:42.300742Z",
     "start_time": "2025-01-22T10:24:42.295629Z"
    }
   },
   "id": "8997d8d6901529cd",
   "execution_count": 177
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bd2fa6fb45664000"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualizing code components"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a161ffc80afd30b0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'functions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[179], line 25\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dot\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Create the block diagram\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m block_diagram \u001B[38;5;241m=\u001B[39m create_block_diagram(\u001B[43mfunctions\u001B[49m, classes)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Render the diagram to a file (PDF or PNG)\u001B[39;00m\n\u001B[1;32m     28\u001B[0m block_diagram\u001B[38;5;241m.\u001B[39mrender(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrepo_design_block_diagram\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpng\u001B[39m\u001B[38;5;124m'\u001B[39m, view\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'functions' is not defined"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def create_block_diagram(functions, classes):\n",
    "    \"\"\"\n",
    "    Create a block diagram using Graphviz.\n",
    "    \"\"\"\n",
    "    dot = Digraph(comment='Repo Design')\n",
    "    \n",
    "    # Add classes as nodes\n",
    "    for class_name in classes:\n",
    "        dot.node(class_name, class_name, shape='box', style='filled', fillcolor='lightblue')\n",
    "    \n",
    "    # Add functions as nodes\n",
    "    for func_name, _ in functions:\n",
    "        dot.node(func_name, func_name, shape='ellipse', style='filled', fillcolor='lightgreen')\n",
    "    \n",
    "    # Add edges based on relationships\n",
    "    for func_name, _ in functions:\n",
    "        for class_name in classes:\n",
    "            dot.edge(class_name, func_name)\n",
    "    \n",
    "    return dot\n",
    "\n",
    "# Create the block diagram\n",
    "block_diagram = create_block_diagram(functions, classes)\n",
    "\n",
    "# Render the diagram to a file (PDF or PNG)\n",
    "block_diagram.render('repo_design_block_diagram', format='png', view=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-22T22:12:20.169970Z",
     "start_time": "2025-01-22T22:12:20.091474Z"
    }
   },
   "id": "1f5f8caf79857b66",
   "execution_count": 179
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "27c7ec7635b34b54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
