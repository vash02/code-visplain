{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-14T10:40:48.028672Z",
     "start_time": "2025-01-14T10:40:44.126380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "langchain-huggingface 0.1.2 requires langchain-core<0.4.0,>=0.3.15, but you have langchain-core 0.2.43 which is incompatible.\r\n",
      "langchain-mistralai 0.2.4 requires langchain-core<0.4.0,>=0.3.27, but you have langchain-core 0.2.43 which is incompatible.\r\n",
      "langchain-ollama 0.2.2 requires langchain-core<0.4.0,>=0.3.27, but you have langchain-core 0.2.43 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community ollama chromadb -q"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./.venv/lib/python3.9/site-packages (0.2.17)\r\n",
      "Requirement already satisfied: langchainhub in ./.venv/lib/python3.9/site-packages (0.1.21)\r\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.9/site-packages (3.2.0)\r\n",
      "Requirement already satisfied: huggingface_hub in ./.venv/lib/python3.9/site-packages (0.24.5)\r\n",
      "Requirement already satisfied: pdfplumber in ./.venv/lib/python3.9/site-packages (0.11.5)\r\n",
      "Requirement already satisfied: faiss-cpu in ./.venv/lib/python3.9/site-packages (1.9.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.9/site-packages (from langchain) (6.0.2)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.9/site-packages (from langchain) (2.0.32)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.venv/lib/python3.9/site-packages (from langchain) (3.10.2)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.venv/lib/python3.9/site-packages (from langchain) (4.0.3)\r\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.43 in ./.venv/lib/python3.9/site-packages (from langchain) (0.2.43)\r\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./.venv/lib/python3.9/site-packages (from langchain) (0.2.4)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.venv/lib/python3.9/site-packages (from langchain) (0.1.147)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.venv/lib/python3.9/site-packages (from langchain) (1.26.4)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./.venv/lib/python3.9/site-packages (from langchain) (2.10.5)\r\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.9/site-packages (from langchain) (2.32.3)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.venv/lib/python3.9/site-packages (from langchain) (8.5.0)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.9/site-packages (from langchainhub) (24.1)\r\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in ./.venv/lib/python3.9/site-packages (from langchainhub) (2.32.0.20241016)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (4.44.0)\r\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (4.66.5)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (2.4.0)\r\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (1.5.2)\r\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\r\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (11.0.0)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from huggingface_hub) (3.15.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface_hub) (2024.6.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface_hub) (4.12.2)\r\n",
      "Requirement already satisfied: pdfminer.six==20231228 in ./.venv/lib/python3.9/site-packages (from pdfplumber) (20231228)\r\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in ./.venv/lib/python3.9/site-packages (from pdfplumber) (4.30.1)\r\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in ./.venv/lib/python3.9/site-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\r\n",
      "Requirement already satisfied: cryptography>=36.0.0 in ./.venv/lib/python3.9/site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.5)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.3.0,>=0.2.43->langchain) (1.33)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\r\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (2.27.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2024.7.4)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.7.24)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.4)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\r\n",
      "Requirement already satisfied: cffi>=1.12 in ./.venv/lib/python3.9/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\r\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.4.0)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\r\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain) (3.0.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\r\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchainhub sentence-transformers huggingface_hub pdfplumber faiss-cpu\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T10:40:49.396600Z",
     "start_time": "2025-01-14T10:40:48.030123Z"
    }
   },
   "id": "4518d315ec2cc4b1",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_ollama in ./.venv/lib/python3.9/site-packages (0.2.2)\r\n",
      "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain_ollama)\r\n",
      "  Obtaining dependency information for langchain-core<0.4.0,>=0.3.27 from https://files.pythonhosted.org/packages/95/4f/fe1de63f6fc1ac7af3ba4ae12d420af1a19f7893b5fcb72856b9fc67f650/langchain_core-0.3.29-py3-none-any.whl.metadata\r\n",
      "  Using cached langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in ./.venv/lib/python3.9/site-packages (from langchain_ollama) (0.4.6)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_ollama) (6.0.2)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_ollama) (1.33)\r\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_ollama) (0.1.147)\r\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_ollama) (24.1)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_ollama) (2.10.5)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_ollama) (8.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain_ollama) (4.12.2)\r\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in ./.venv/lib/python3.9/site-packages (from ollama<1,>=0.4.4->langchain_ollama) (0.27.0)\r\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.9/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain_ollama) (4.4.0)\r\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.9/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain_ollama) (2024.7.4)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.9/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain_ollama) (1.0.5)\r\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.9/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain_ollama) (3.7)\r\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.9/site-packages (from httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain_ollama) (1.3.1)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain_ollama) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_ollama) (3.0.0)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_ollama) (3.10.6)\r\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_ollama) (2.32.3)\r\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_ollama) (1.0.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain_ollama) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain_ollama) (2.27.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_ollama) (3.3.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2->langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_ollama) (2.2.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.9/site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama<1,>=0.4.4->langchain_ollama) (1.2.2)\r\n",
      "Using cached langchain_core-0.3.29-py3-none-any.whl (411 kB)\r\n",
      "Installing collected packages: langchain-core\r\n",
      "  Attempting uninstall: langchain-core\r\n",
      "    Found existing installation: langchain-core 0.2.43\r\n",
      "    Uninstalling langchain-core-0.2.43:\r\n",
      "      Successfully uninstalled langchain-core-0.2.43\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "langchain 0.2.17 requires langchain-core<0.3.0,>=0.2.43, but you have langchain-core 0.3.29 which is incompatible.\r\n",
      "langchain-community 0.2.11 requires langchain-core<0.3.0,>=0.2.27, but you have langchain-core 0.3.29 which is incompatible.\r\n",
      "langchain-google-genai 1.0.8 requires langchain-core<0.3,>=0.2.17, but you have langchain-core 0.3.29 which is incompatible.\r\n",
      "langchain-openai 0.1.20 requires langchain-core<0.3.0,>=0.2.26, but you have langchain-core 0.3.29 which is incompatible.\r\n",
      "langchain-text-splitters 0.2.4 requires langchain-core<0.3.0,>=0.2.38, but you have langchain-core 0.3.29 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed langchain-core-0.3.29\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain_ollama"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T10:40:51.140137Z",
     "start_time": "2025-01-14T10:40:49.397555Z"
    }
   },
   "id": "bf6f73fbe456df72",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaibhav/PycharmProjects/llm-visplain/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/var/folders/q3/mq53py8958ndz_blwtj161xm0000gn/T/ipykernel_90801/2855599551.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.llms import ollama\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import ollama\n",
    "from langchain_community.vectorstores import chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_importer\n",
    "from langchain.prompts import PromptTemplate\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T10:40:52.233198Z",
     "start_time": "2025-01-14T10:40:51.141789Z"
    }
   },
   "id": "a4707cc78ba0c3f3",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Executing Embedding generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5955129354568e9c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 0, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Improving Language Understanding\\nby Generative Pre-Training\\nAlecRadford KarthikNarasimhan TimSalimans IlyaSutskever\\nOpenAI OpenAI OpenAI OpenAI\\nalec@openai.com karthikn@openai.com tim@openai.com ilyasu@openai.com\\nAbstract\\nNatural language understanding comprises a wide range of diverse tasks such\\nas textual entailment, question answering, semantic similarity assessment, and\\ndocument classification. Although large unlabeled text corpora are abundant,\\nlabeleddataforlearningthesespecifictasksisscarce,makingitchallengingfor\\ndiscriminativelytrainedmodelstoperformadequately. Wedemonstratethatlarge\\ngainsonthesetaskscanberealizedbygenerativepre-trainingofalanguagemodel\\nonadiversecorpusofunlabeledtext,followedbydiscriminativefine-tuningoneach\\nspecifictask. Incontrasttopreviousapproaches,wemakeuseoftask-awareinput\\ntransformations during fine-tuning to achieve effective transfer while requiring\\nminimalchangestothemodelarchitecture. Wedemonstratetheeffectivenessof\\nourapproachonawiderangeofbenchmarksfornaturallanguageunderstanding.\\nOurgeneraltask-agnosticmodeloutperformsdiscriminativelytrainedmodelsthat\\nusearchitecturesspecificallycraftedforeachtask,significantlyimprovinguponthe\\nstateoftheartin9outofthe12tasksstudied. Forinstance,weachieveabsolute\\nimprovementsof8.9%oncommonsensereasoning(StoriesClozeTest),5.7%on\\nquestionanswering(RACE),and1.5%ontextualentailment(MultiNLI).\\n1 Introduction\\nTheabilitytolearneffectivelyfromrawtextiscrucialtoalleviatingthedependenceonsupervised\\nlearning in natural language processing (NLP). Most deep learning methods require substantial\\namountsofmanuallylabeleddata,whichrestrictstheirapplicabilityinmanydomainsthatsuffer\\nfromadearthofannotatedresources[61]. Inthesesituations,modelsthatcanleveragelinguistic\\ninformationfromunlabeleddataprovideavaluablealternativetogatheringmoreannotation,which\\ncan be time-consuming and expensive. Further, even in cases where considerable supervision\\nis available, learning good representations in an unsupervised fashion can provide a significant\\nperformanceboost. Themostcompellingevidenceforthissofarhasbeentheextensiveuseofpre-\\ntrainedwordembeddings[10,39,42]toimproveperformanceonarangeofNLPtasks[8,11,26,45].\\nLeveragingmorethanword-levelinformationfromunlabeledtext,however,ischallengingfortwo\\nmainreasons. First,itisunclearwhattypeofoptimizationobjectivesaremosteffectiveatlearning\\ntext representations that are useful for transfer. Recent research has looked at various objectives\\nsuchaslanguagemodeling[44],machinetranslation[38],anddiscoursecoherence[22],witheach\\nmethod outperforming the others on different tasks.1 Second, there is no consensus on the most\\neffectivewaytotransfertheselearnedrepresentationstothetargettask. Existingtechniquesinvolve\\nacombinationofmakingtask-specificchangestothemodelarchitecture[43,44],usingintricate\\nlearningschemes[21]andaddingauxiliarylearningobjectives[50]. Theseuncertaintieshavemade\\nitdifficulttodevelopeffectivesemi-supervisedlearningapproachesforlanguageprocessing.\\n1https://gluebenchmark.com/leaderboard\\nPreprint.Workinprogress.'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='In this paper, we explore a semi-supervised approach for language understanding tasks using a\\ncombinationofunsupervisedpre-trainingandsupervisedfine-tuning. Ourgoalistolearnauniversal\\nrepresentationthattransferswithlittleadaptationtoawiderangeoftasks. Weassumeaccessto\\na large corpus of unlabeled text and several datasets with manually annotated training examples\\n(targettasks). Oursetupdoesnotrequirethesetargettaskstobeinthesamedomainastheunlabeled\\ncorpus. Weemployatwo-stagetrainingprocedure. First,weusealanguagemodelingobjectiveon\\ntheunlabeleddatatolearntheinitialparametersofaneuralnetworkmodel. Subsequently,weadapt\\ntheseparameterstoatargettaskusingthecorrespondingsupervisedobjective.\\nForourmodelarchitecture,weusetheTransformer[62],whichhasbeenshowntoperformstronglyon\\nvarioustaskssuchasmachinetranslation[62],documentgeneration[34],andsyntacticparsing[29].\\nThismodelchoiceprovidesuswithamorestructuredmemoryforhandlinglong-termdependenciesin\\ntext,comparedtoalternativeslikerecurrentnetworks,resultinginrobusttransferperformanceacross\\ndiversetasks. Duringtransfer,weutilizetask-specificinputadaptationsderivedfromtraversal-style\\napproaches[52],whichprocessstructuredtextinputasasinglecontiguoussequenceoftokens. As\\nwedemonstrateinourexperiments,theseadaptationsenableustofine-tuneeffectivelywithminimal\\nchangestothearchitectureofthepre-trainedmodel.\\nWeevaluateourapproachonfourtypesoflanguageunderstandingtasks–naturallanguageinference,\\nquestionanswering,semanticsimilarity,andtextclassification. Ourgeneraltask-agnosticmodel\\noutperformsdiscriminativelytrainedmodelsthatemployarchitecturesspecificallycraftedforeach\\ntask,significantlyimprovinguponthestateoftheartin9outofthe12tasksstudied. Forinstance,\\nweachieveabsoluteimprovementsof8.9%oncommonsensereasoning(StoriesClozeTest)[40],\\n5.7%onquestionanswering(RACE)[30],1.5%ontextualentailment(MultiNLI)[66]and5.5%on\\ntherecentlyintroducedGLUEmulti-taskbenchmark[64]. Wealsoanalyzedzero-shotbehaviors\\nofthepre-trainedmodelonfourdifferentsettingsanddemonstratethatitacquiresusefullinguistic\\nknowledgefordownstreamtasks.\\n2 RelatedWork\\nSemi-supervisedlearningforNLP Ourworkbroadlyfallsunderthecategoryofsemi-supervised\\nlearningfornaturallanguage. Thisparadigmhasattractedsignificantinterest,withapplicationsto\\ntaskslikesequencelabeling[24,33,57]ortextclassification[41,70]. Theearliestapproachesused\\nunlabeleddatatocomputeword-levelorphrase-levelstatistics,whichwerethenusedasfeaturesina\\nsupervisedmodel[33]. Overthelastfewyears,researchershavedemonstratedthebenefitsofusing\\nwordembeddings[11,39,42],whicharetrainedonunlabeledcorpora,toimproveperformanceona\\nvarietyoftasks[8,11,26,45]. Theseapproaches,however,mainlytransferword-levelinformation,\\nwhereasweaimtocapturehigher-levelsemantics.\\nRecentapproacheshaveinvestigatedlearningandutilizingmorethanword-levelsemanticsfrom\\nunlabeleddata. Phrase-levelorsentence-levelembeddings,whichcanbetrainedusinganunlabeled\\ncorpus,havebeenusedtoencodetextintosuitablevectorrepresentationsforvarioustargettasks[28,\\n32,1,36,22,12,56,31].\\nUnsupervisedpre-training Unsupervisedpre-trainingisaspecialcaseofsemi-supervisedlearning\\nwhere the goal is to find a good initialization point instead of modifying the supervised learning\\nobjective. Earlyworksexploredtheuseofthetechniqueinimageclassification[20,49,63]and'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 1, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='whereasweaimtocapturehigher-levelsemantics.\\nRecentapproacheshaveinvestigatedlearningandutilizingmorethanword-levelsemanticsfrom\\nunlabeleddata. Phrase-levelorsentence-levelembeddings,whichcanbetrainedusinganunlabeled\\ncorpus,havebeenusedtoencodetextintosuitablevectorrepresentationsforvarioustargettasks[28,\\n32,1,36,22,12,56,31].\\nUnsupervisedpre-training Unsupervisedpre-trainingisaspecialcaseofsemi-supervisedlearning\\nwhere the goal is to find a good initialization point instead of modifying the supervised learning\\nobjective. Earlyworksexploredtheuseofthetechniqueinimageclassification[20,49,63]and\\nregressiontasks[3]. Subsequentresearch[15]demonstratedthatpre-trainingactsasaregularization\\nscheme, enabling better generalization in deep neural networks. In recent work, the method has\\nbeenusedtohelptraindeepneuralnetworksonvarioustaskslikeimageclassification[69],speech\\nrecognition[68],entitydisambiguation[17]andmachinetranslation[48].\\nTheclosestlineofworktooursinvolvespre-traininganeuralnetworkusingalanguagemodeling\\nobjectiveandthenfine-tuningitonatargettaskwithsupervision. Daietal.[13]andHowardand\\nRuder[21]followthismethodtoimprovetextclassification. However,althoughthepre-training\\nphasehelpscapturesomelinguisticinformation,theirusageofLSTMmodelsrestrictstheirprediction\\nabilitytoashortrange. Incontrast,ourchoiceoftransformernetworksallowsustocapturelonger-\\nrange linguistic structure, as demonstrated in our experiments. Further, we also demonstrate the\\neffectivenessofourmodelonawiderrangeoftasksincludingnaturallanguageinference,paraphrase\\ndetectionandstorycompletion. Otherapproaches[43,44,38]usehiddenrepresentationsfroma\\n2'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='pre-trainedlanguageormachinetranslationmodelasauxiliaryfeatureswhiletrainingasupervised\\nmodelonthetargettask. Thisinvolvesasubstantialamountofnewparametersforeachseparate\\ntargettask,whereaswerequireminimalchangestoourmodelarchitectureduringtransfer.\\nAuxiliarytrainingobjectives Addingauxiliaryunsupervisedtrainingobjectivesisanalternative\\nformofsemi-supervisedlearning. EarlyworkbyCollobertandWeston[10]usedawidevarietyof\\nauxiliaryNLPtaskssuchasPOStagging,chunking,namedentityrecognition,andlanguagemodeling\\ntoimprovesemanticrolelabeling. Morerecently,Rei[50]addedanauxiliarylanguagemodeling\\nobjectivetotheirtargettaskobjectiveanddemonstratedperformancegainsonsequencelabeling\\ntasks. Ourexperimentsalsouseanauxiliaryobjective,butasweshow,unsupervisedpre-training\\nalreadylearnsseverallinguisticaspectsrelevanttotargettasks.\\n3 Framework\\nOurtrainingprocedureconsistsoftwostages. Thefirststageislearningahigh-capacitylanguage\\nmodelonalargecorpusoftext. Thisisfollowedbyafine-tuningstage,whereweadaptthemodelto\\nadiscriminativetaskwithlabeleddata.\\n3.1 Unsupervisedpre-training\\nGivenanunsupervisedcorpusoftokensU ={u ,...,u },weuseastandardlanguagemodeling\\n1 n\\nobjectivetomaximizethefollowinglikelihood:\\n(cid:88)\\nL (U)= logP(u |u ,...,u ;Θ) (1)\\n1 i i−k i−1\\ni\\nwherekisthesizeofthecontextwindow,andtheconditionalprobabilityP ismodeledusinganeural\\nnetworkwithparametersΘ. Theseparametersaretrainedusingstochasticgradientdescent[51].\\nInourexperiments,weuseamulti-layerTransformerdecoder[34]forthelanguagemodel,whichis\\navariantofthetransformer[62]. Thismodelappliesamulti-headedself-attentionoperationoverthe\\ninputcontexttokensfollowedbyposition-wisefeedforwardlayerstoproduceanoutputdistribution\\novertargettokens:\\nh =UW +W\\n0 e p\\nh l =transformer_block(h l−1)∀i∈[1,n] (2)\\nP(u)=softmax(h WT)\\nn e\\nwhereU =(u ,...,u )isthecontextvectoroftokens,nisthenumberoflayers,W isthetoken\\n−k −1 e\\nembeddingmatrix,andW isthepositionembeddingmatrix.\\np\\n3.2 Supervisedfine-tuning\\nAftertrainingthemodelwiththeobjectiveinEq.1,weadapttheparameterstothesupervisedtarget\\ntask. WeassumealabeleddatasetC,whereeachinstanceconsistsofasequenceofinputtokens,\\nx1,...,xm, alongwithalabely. Theinputsarepassedthroughourpre-trainedmodeltoobtain\\nthefinaltransformerblock’sactivationhm,whichisthenfedintoanaddedlinearoutputlayerwith\\nl\\nparametersW topredicty:\\ny\\nP(y|x1,...,xm)=softmax(hmW ). (3)\\nl y\\nThisgivesusthefollowingobjectivetomaximize:\\n(cid:88)\\nL (C)= logP(y|x1,...,xm). (4)\\n2\\n(x,y)\\nWeadditionallyfoundthatincludinglanguagemodelingasanauxiliaryobjectivetothefine-tuning\\nhelped learning by (a) improving generalization of the supervised model, and (b) accelerating\\nconvergence. Thisisinlinewithpriorwork[50,43],whoalsoobservedimprovedperformancewith\\nsuchanauxiliaryobjective. Specifically,weoptimizethefollowingobjective(withweightλ):\\nL (C)=L (C)+λ∗L (C) (5)\\n3 2 1\\nOverall,theonlyextraparameterswerequireduringfine-tuningareW ,andembeddingsfordelimiter\\ny'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 2, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='l y\\nThisgivesusthefollowingobjectivetomaximize:\\n(cid:88)\\nL (C)= logP(y|x1,...,xm). (4)\\n2\\n(x,y)\\nWeadditionallyfoundthatincludinglanguagemodelingasanauxiliaryobjectivetothefine-tuning\\nhelped learning by (a) improving generalization of the supervised model, and (b) accelerating\\nconvergence. Thisisinlinewithpriorwork[50,43],whoalsoobservedimprovedperformancewith\\nsuchanauxiliaryobjective. Specifically,weoptimizethefollowingobjective(withweightλ):\\nL (C)=L (C)+λ∗L (C) (5)\\n3 2 1\\nOverall,theonlyextraparameterswerequireduringfine-tuningareW ,andembeddingsfordelimiter\\ny\\ntokens(describedbelowinSection3.3).\\n3'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 3, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Figure 1: (left) Transformer architecture and training objectives used in this work. (right) Input\\ntransformations for fine-tuning on different tasks. We convert all structured inputs into token\\nsequencestobeprocessedbyourpre-trainedmodel,followedbyalinear+softmaxlayer.\\n3.3 Task-specificinputtransformations\\nFor some tasks, like text classification, we can directly fine-tune our model as described above.\\nCertainothertasks, likequestionansweringortextualentailment, havestructuredinputssuchas\\norderedsentencepairs,ortripletsofdocument,question,andanswers. Sinceourpre-trainedmodel\\nwastrainedoncontiguoussequencesoftext,werequiresomemodificationstoapplyittothesetasks.\\nPreviousworkproposedlearningtaskspecificarchitecturesontopoftransferredrepresentations[44].\\nSuchanapproachre-introducesasignificantamountoftask-specificcustomizationanddoesnot\\nusetransferlearningfortheseadditionalarchitecturalcomponents. Instead,weuseatraversal-style\\napproach [52], where we convert structured inputs into an ordered sequence that our pre-trained\\nmodelcanprocess. Theseinputtransformationsallowustoavoidmakingextensivechangestothe\\narchitectureacrosstasks. Weprovideabriefdescriptionoftheseinputtransformationsbelowand\\nFigure1providesavisualillustration. Alltransformationsincludeaddingrandomlyinitializedstart\\nandendtokens((cid:104)s(cid:105),(cid:104)e(cid:105)).\\nTextualentailment Forentailmenttasks,weconcatenatethepremisepandhypothesishtoken\\nsequences,withadelimitertoken($)inbetween.\\nSimilarity Forsimilaritytasks,thereisnoinherentorderingofthetwosentencesbeingcompared.\\nToreflectthis,wemodifytheinputsequencetocontainbothpossiblesentenceorderings(witha\\ndelimiterinbetween)andprocesseachindependentlytoproducetwosequencerepresentationshm\\nl\\nwhichareaddedelement-wisebeforebeingfedintothelinearoutputlayer.\\nQuestion Answering and Commonsense Reasoning For these tasks, we are given a context\\ndocumentz,aquestionq,andasetofpossibleanswers{a }. Weconcatenatethedocumentcontext\\nk\\nandquestionwitheachpossibleanswer,addingadelimitertokeninbetweentoget[z;q;$;a ]. Each\\nk\\nofthesesequencesareprocessedindependentlywithourmodelandthennormalizedviaasoftmax\\nlayertoproduceanoutputdistributionoverpossibleanswers.\\n4 Experiments\\n4.1 Setup\\nUnsupervisedpre-training WeusetheBooksCorpusdataset[71]fortrainingthelanguagemodel.\\nIt contains over 7,000 unique unpublished books from a variety of genres including Adventure,\\nFantasy, andRomance. Crucially, itcontainslongstretchesofcontiguoustext, whichallowsthe\\ngenerativemodeltolearntoconditiononlong-rangeinformation. Analternativedataset, the1B\\nWordBenchmark,whichisusedbyasimilarapproach,ELMo[44],isapproximatelythesamesize\\n4'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Table1: Alistofthedifferenttasksanddatasetsusedinourexperiments.\\nTask Datasets\\nNaturallanguageinference SNLI[5],MultiNLI[66],QuestionNLI[64],RTE[4],SciTail[25]\\nQuestionAnswering RACE[30],StoryCloze[40]\\nSentencesimilarity MSRParaphraseCorpus[14],QuoraQuestionPairs[9],STSBenchmark[6]\\nClassification StanfordSentimentTreebank-2[54],CoLA[65]\\nbutisshuffledatasentencelevel-destroyinglong-rangestructure. Ourlanguagemodelachievesa\\nverylowtokenlevelperplexityof18.4onthiscorpus.\\nModelspecifications Ourmodellargelyfollowstheoriginaltransformerwork[62]. Wetraineda\\n12-layerdecoder-onlytransformerwithmaskedself-attentionheads(768dimensionalstatesand12\\nattentionheads). Fortheposition-wisefeed-forwardnetworks,weused3072dimensionalinnerstates.\\nWeusedtheAdamoptimizationscheme[27]withamaxlearningrateof2.5e-4. Thelearningrate\\nwasincreasedlinearlyfromzerooverthefirst2000updatesandannealedto0usingacosineschedule.\\nWetrainfor100epochsonminibatchesof64randomlysampled,contiguoussequencesof512tokens.\\nSince layernorm [2] is used extensively throughout the model, a simple weight initialization of\\nN(0,0.02)wassufficient. Weusedabytepairencoding(BPE)vocabularywith40,000merges[53]\\nand residual, embedding, and attention dropouts with a rate of 0.1 for regularization. We also\\nemployedamodifiedversionofL2regularizationproposedin[37],withw =0.01onallnonbiasor\\ngainweights. Fortheactivationfunction,weusedtheGaussianErrorLinearUnit(GELU)[18]. We\\nusedlearnedpositionembeddingsinsteadofthesinusoidalversionproposedintheoriginalwork.\\nWeusetheftfylibrary2 tocleantherawtextinBooksCorpus, standardizesomepunctuationand\\nwhitespace,andusethespaCytokenizer.3\\nFine-tuning details Unless specified, we reuse the hyperparameter settings from unsupervised\\npre-training. Weadddropouttotheclassifierwitharateof0.1. Formosttasks,weusealearningrate\\nof6.25e-5andabatchsizeof32. Ourmodelfinetunesquicklyand3epochsoftrainingwassufficient\\nformostcases. Weusealinearlearningratedecayschedulewithwarmupover0.2%oftraining. λ\\nwassetto0.5.\\n4.2 Supervisedfine-tuning\\nWe perform experiments on a variety of supervised tasks including natural language inference,\\nquestionanswering,semanticsimilarity,andtextclassification. Someofthesetasksareavailable\\naspartoftherecentlyreleasedGLUEmulti-taskbenchmark[64],whichwemakeuseof. Figure1\\nprovidesanoverviewofallthetasksanddatasets.\\nNaturalLanguageInference Thetaskofnaturallanguageinference(NLI),alsoknownasrecog-\\nnizingtextualentailment,involvesreadingapairofsentencesandjudgingtherelationshipbetween\\nthem from one of entailment, contradiction or neutral. Although there has been a lot of\\nrecentinterest[58,35,44],thetaskremainschallengingduetothepresenceofawidevarietyof\\nphenomenalikelexicalentailment,coreference,andlexicalandsyntacticambiguity. Weevaluate\\nonfivedatasetswithdiversesources,includingimagecaptions(SNLI),transcribedspeech,popular\\nfiction,andgovernmentreports(MNLI),Wikipediaarticles(QNLI),scienceexams(SciTail)ornews\\narticles(RTE).\\nTable2detailsvariousresultsonthedifferentNLItasksforourmodelandpreviousstate-of-the-art\\napproaches. Ourmethodsignificantlyoutperformsthebaselinesonfourofthefivedatasets,achieving'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 4, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='them from one of entailment, contradiction or neutral. Although there has been a lot of\\nrecentinterest[58,35,44],thetaskremainschallengingduetothepresenceofawidevarietyof\\nphenomenalikelexicalentailment,coreference,andlexicalandsyntacticambiguity. Weevaluate\\nonfivedatasetswithdiversesources,includingimagecaptions(SNLI),transcribedspeech,popular\\nfiction,andgovernmentreports(MNLI),Wikipediaarticles(QNLI),scienceexams(SciTail)ornews\\narticles(RTE).\\nTable2detailsvariousresultsonthedifferentNLItasksforourmodelandpreviousstate-of-the-art\\napproaches. Ourmethodsignificantlyoutperformsthebaselinesonfourofthefivedatasets,achieving\\nabsoluteimprovementsofupto1.5%onMNLI,5%onSciTail,5.8%onQNLIand0.6%onSNLI\\noverthepreviousbestresults. Thisdemonstratesourmodel’sabilitytobetterreasonovermultiple\\nsentences, and handle aspects of linguistic ambiguity. On RTE, one of the smaller datasets we\\nevaluateon(2490examples),weachieveanaccuracyof56%,whichisbelowthe61.7%reportedbya\\nmulti-taskbiLSTMmodel. GiventhestrongperformanceofourapproachonlargerNLIdatasets,itis\\nlikelyourmodelwillbenefitfrommulti-tasktrainingaswellbutwehavenotexploredthiscurrently.\\n2https://ftfy.readthedocs.io/en/latest/\\n3https://spacy.io/\\n5'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Table2: Experimentalresultsonnaturallanguageinferencetasks,comparingourmodelwithcurrent\\nstate-of-the-artmethods. 5xindicatesanensembleof5models. Alldatasetsuseaccuracyasthe\\nevaluationmetric.\\nMethod MNLI-m MNLI-mm SNLI SciTail QNLI RTE\\nESIM+ELMo[44](5x) - - 89.3 - - -\\nCAFE[58](5x) 80.2 79.0 89.3 - - -\\nStochasticAnswerNetwork[35](3x) 80.6 80.1 - - - -\\nCAFE[58] 78.7 77.9 88.5 83.3\\nGenSen[64] 71.4 71.3 - - 82.3 59.2\\nMulti-taskBiLSTM+Attn[64] 72.2 72.1 - - 82.1 61.7\\nFinetunedTransformerLM(ours) 82.1 81.4 89.9 88.3 88.1 56.0\\nTable3: Resultsonquestionansweringandcommonsensereasoning,comparingourmodelwith\\ncurrentstate-of-the-artmethods.. 9xmeansanensembleof9models.\\nMethod StoryCloze RACE-m RACE-h RACE\\nval-LS-skip[55] 76.5 - - -\\nHiddenCoherenceModel[7] 77.6 - - -\\nDynamicFusionNet[67](9x) - 55.6 49.4 51.2\\nBiAttentionMRU[59](9x) - 60.2 50.3 53.3\\nFinetunedTransformerLM(ours) 86.5 62.9 57.4 59.0\\nQuestionansweringandcommonsensereasoning Anothertaskthatrequiresaspectsofsingle\\nandmulti-sentencereasoningisquestionanswering. WeusetherecentlyreleasedRACEdataset[30],\\nconsistingofEnglishpassageswithassociatedquestionsfrommiddleandhighschoolexams. This\\ncorpushasbeenshowntocontainmorereasoningtypequestionsthatotherdatasetslikeCNN[19]or\\nSQuaD[47],providingtheperfectevaluationforourmodelwhichistrainedtohandlelong-range\\ncontexts. Inaddition,weevaluateontheStoryClozeTest[40],whichinvolvesselectingthecorrect\\nendingtomulti-sentencestoriesfromtwooptions. Onthesetasks,ourmodelagainoutperformsthe\\npreviousbestresultsbysignificantmargins-upto8.9%onStoryCloze,and5.7%overallonRACE.\\nThisdemonstratestheabilityofourmodeltohandlelong-rangecontextseffectively.\\nSemanticSimilarity Semanticsimilarity(orparaphrasedetection)tasksinvolvepredictingwhether\\ntwosentencesaresemanticallyequivalentornot. Thechallengeslieinrecognizingrephrasingof\\nconcepts,understandingnegation,andhandlingsyntacticambiguity. Weusethreedatasetsforthis\\ntask – the Microsoft Paraphrase corpus (MRPC) [14] (collected from news sources), the Quora\\nQuestion Pairs (QQP) dataset [9], and the Semantic Textual Similarity benchmark (STS-B) [6].\\nWeobtainstate-of-the-artresultsontwoofthethreesemanticsimilaritytasks(Table4)witha1\\npointabsolutegainonSTS-B.TheperformancedeltaonQQPissignificant,witha4.2%absolute\\nimprovementoverSingle-taskBiLSTM+ELMo+Attn.\\nClassification Finally, we also evaluate on two different text classification tasks. The Corpus\\nof Linguistic Acceptability (CoLA) [65] contains expert judgements on whether a sentence is\\ngrammaticalornot,andteststheinnatelinguisticbiasoftrainedmodels. TheStanfordSentiment\\nTreebank(SST-2)[54],ontheotherhand,isastandardbinaryclassificationtask. Ourmodelobtains\\nanscoreof45.4onCoLA,whichisanespeciallybigjumpoverthepreviousbestresultof35.0,\\nshowcasingtheinnatelinguisticbiaslearnedbyourmodel. Themodelalsoachieves91.3%accuracy'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 5, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='pointabsolutegainonSTS-B.TheperformancedeltaonQQPissignificant,witha4.2%absolute\\nimprovementoverSingle-taskBiLSTM+ELMo+Attn.\\nClassification Finally, we also evaluate on two different text classification tasks. The Corpus\\nof Linguistic Acceptability (CoLA) [65] contains expert judgements on whether a sentence is\\ngrammaticalornot,andteststheinnatelinguisticbiasoftrainedmodels. TheStanfordSentiment\\nTreebank(SST-2)[54],ontheotherhand,isastandardbinaryclassificationtask. Ourmodelobtains\\nanscoreof45.4onCoLA,whichisanespeciallybigjumpoverthepreviousbestresultof35.0,\\nshowcasingtheinnatelinguisticbiaslearnedbyourmodel. Themodelalsoachieves91.3%accuracy\\nonSST-2,whichiscompetitivewiththestate-of-the-artresults. Wealsoachieveanoverallscoreof\\n72.8ontheGLUEbenchmark,whichissignificantlybetterthanthepreviousbestof68.9.\\n6'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 6, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Table4: Semanticsimilarityandclassificationresults,comparingourmodelwithcurrentstate-of-the-\\nartmethods. AlltaskevaluationsinthistableweredoneusingtheGLUEbenchmark. (mc=Mathews\\ncorrelation,acc=Accuracy,pc=Pearsoncorrelation)\\nMethod Classification SemanticSimilarity GLUE\\nCoLA SST2 MRPC STSB QQP\\n(mc) (acc) (F1) (pc) (F1)\\nSparsebytemLSTM[16] - 93.2 - - - -\\nTF-KLD[23] - - 86.0 - - -\\nECNU(mixedensemble)[60] - - - 81.0 - -\\nSingle-taskBiLSTM+ELMo+Attn[64] 35.0 90.2 80.2 55.5 66.1 64.8\\nMulti-taskBiLSTM+ELMo+Attn[64] 18.9 91.6 83.5 72.8 63.3 68.9\\nFinetunedTransformerLM(ours) 45.4 91.3 82.3 82.0 70.3 72.8\\nOverall,ourapproachachievesnewstate-of-the-artresultsin9outofthe12datasetsweevaluate\\non,outperformingensemblesinmanycases. Ourresultsalsoindicatethatourapproachworkswell\\nacrossdatasetsofdifferentsizes,fromsmallerdatasetssuchasSTS-B(≈5.7ktrainingexamples)–\\ntothelargestone–SNLI(≈550ktrainingexamples).\\n5 Analysis\\nImpactofnumberoflayerstransferred Weobservedtheimpactoftransferringavariablenumber\\noflayersfromunsupervisedpre-trainingtothesupervisedtargettask. Figure2(left)illustratesthe\\nperformanceofourapproachonMultiNLIandRACEasafunctionofthenumberoflayerstransferred.\\nWeobservethestandardresultthattransferringembeddingsimprovesperformanceandthateach\\ntransformerlayerprovidesfurtherbenefitsupto9%forfulltransferonMultiNLI.Thisindicatesthat\\neachlayerinthepre-trainedmodelcontainsusefulfunctionalityforsolvingtargettasks.\\nFigure 2: (left) Effect of transferring increasing number of layers from the pre-trained language\\nmodel on RACE and MultiNLI. (right) Plot showing the evolution of zero-shot performance on\\ndifferenttasksasafunctionofLMpre-trainingupdates. Performancepertaskisnormalizedbetween\\narandomguessbaselineandthecurrentstate-of-the-artwithasinglemodel.\\nZero-shotBehaviors We’dliketobetterunderstandwhylanguagemodelpre-trainingoftransform-\\nersiseffective. Ahypothesisisthattheunderlyinggenerativemodellearnstoperformmanyofthe\\ntasksweevaluateoninordertoimproveitslanguagemodelingcapabilityandthatthemorestructured\\n7'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='Table5: Analysisofvariousmodelablationsondifferenttasks. Avg. scoreisaunweightedaverage\\nofalltheresults. (mc=Mathewscorrelation,acc=Accuracy,pc=Pearsoncorrelation)\\nMethod Avg.Score CoLA SST2 MRPC STSB QQP MNLI QNLI RTE\\n(mc) (acc) (F1) (pc) (F1) (acc) (acc) (acc)\\nTransformerw/auxLM(full) 74.7 45.4 91.3 82.3 82.0 70.3 81.8 88.1 56.0\\nTransformerw/opre-training 59.9 18.9 84.0 79.4 30.9 65.5 75.7 71.2 53.8\\nTransformerw/oauxLM 75.0 47.9 92.0 84.9 83.2 69.8 81.1 86.9 54.4\\nLSTMw/auxLM 69.1 30.3 90.5 83.2 71.8 68.1 73.7 81.1 54.6\\nattentionalmemoryofthetransformerassistsintransfercomparedtoLSTMs. Wedesignedaseries\\nofheuristicsolutionsthatusetheunderlyinggenerativemodeltoperformtaskswithoutsupervised\\nfinetuning. Wevisualizetheeffectivenessoftheseheuristicsolutionsoverthecourseofgenerative\\npre-traininginFig2(right). Weobservetheperformanceoftheseheuristicsisstableandsteadily\\nincreasesovertrainingsuggestingthatgenerativepretrainingsupportsthelearningofawidevariety\\noftaskrelevantfunctionality. WealsoobservetheLSTMexhibitshighervarianceinitszero-shot\\nperformancesuggestingthattheinductivebiasoftheTransformerarchitectureassistsintransfer.\\nForCoLA(linguisticacceptability),examplesarescoredastheaveragetokenlog-probabilitythe\\ngenerativemodelassignsandpredictionsaremadebythresholding. ForSST-2(sentimentanalysis),\\nweappendthetokenverytoeachexampleandrestrictthelanguagemodel’soutputdistributiontoonly\\nthewordspositiveandnegativeandguessthetokenitassignshigherprobabilitytoastheprediction.\\nForRACE(questionanswering),wepicktheanswerthegenerativemodelassignsthehighestaverage\\ntokenlog-probabilitywhenconditionedonthedocumentandquestion. ForDPRD[46](winograd\\nschemas),wereplacethedefinitepronounwiththetwopossiblereferrentsandpredicttheresolution\\nthatthegenerativemodelassignshigheraveragetokenlog-probabilitytotherestofthesequence\\nafterthesubstitution.\\nAblation studies We perform three different ablation studies (Table 5). First, we examine the\\nperformanceofourmethodwithouttheauxiliaryLMobjectiveduringfine-tuning. Weobservethat\\ntheauxiliaryobjectivehelpsontheNLItasksandQQP.Overall,thetrendsuggeststhatlargerdatasets\\nbenefitfromtheauxiliaryobjectivebutsmallerdatasetsdonot. Second,weanalyzetheeffectofthe\\nTransformerbycomparingitwithasinglelayer2048unitLSTMusingthesameframework. We\\nobservea5.6averagescoredropwhenusingtheLSTMinsteadoftheTransformer. TheLSTMonly\\noutperformstheTransformerononedataset–MRPC.Finally,wealsocomparewithourtransformer\\narchitecturedirectlytrainedonsupervisedtargettasks,withoutpre-training. Weobservethatthelack\\nofpre-traininghurtsperformanceacrossallthetasks,resultingina14.8%decreasecomparedtoour\\nfullmodel.\\n6 Conclusion\\nWe introduced a framework for achieving strong natural language understanding with a single\\ntask-agnosticmodelthroughgenerativepre-traininganddiscriminativefine-tuning. Bypre-training\\non a diverse corpus with long stretches of contiguous text our model acquires significant world\\nknowledgeandabilitytoprocesslong-rangedependencieswhicharethensuccessfullytransferredto\\nsolvingdiscriminativetaskssuchasquestionanswering,semanticsimilarityassessment,entailment'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 7, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='outperformstheTransformerononedataset–MRPC.Finally,wealsocomparewithourtransformer\\narchitecturedirectlytrainedonsupervisedtargettasks,withoutpre-training. Weobservethatthelack\\nofpre-traininghurtsperformanceacrossallthetasks,resultingina14.8%decreasecomparedtoour\\nfullmodel.\\n6 Conclusion\\nWe introduced a framework for achieving strong natural language understanding with a single\\ntask-agnosticmodelthroughgenerativepre-traininganddiscriminativefine-tuning. Bypre-training\\non a diverse corpus with long stretches of contiguous text our model acquires significant world\\nknowledgeandabilitytoprocesslong-rangedependencieswhicharethensuccessfullytransferredto\\nsolvingdiscriminativetaskssuchasquestionanswering,semanticsimilarityassessment,entailment\\ndetermination, and text classification, improving the state of the art on 9 of the 12 datasets we\\nstudy. Using unsupervised (pre-)training to boost performance on discriminative tasks has long\\nbeenanimportantgoalofMachineLearningresearch. Ourworksuggeststhatachievingsignificant\\nperformancegainsisindeedpossible,andoffershintsastowhatmodels(Transformers)anddatasets\\n(textwithlongrangedependencies)workbestwiththisapproach. Wehopethatthiswillhelpenable\\nnewresearchintounsupervisedlearning,forbothnaturallanguageunderstandingandotherdomains,\\nfurtherimprovingourunderstandingofhowandwhenunsupervisedlearningworks.\\nReferences\\n[1] S.Arora,Y.Liang,andT.Ma. Asimplebuttough-to-beatbaselineforsentenceembeddings. 2016.\\n8'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='[2] J.L.Ba,J.R.Kiros,andG.E.Hinton. Layernormalization. arXivpreprintarXiv:1607.06450,2016.\\n[3] Y.Bengio,P.Lamblin,D.Popovici,andH.Larochelle. Greedylayer-wisetrainingofdeepnetworks. In\\nAdvancesinneuralinformationprocessingsystems,pages153–160,2007.\\n[4] L.Bentivogli,P.Clark,I.Dagan,andD.Giampiccolo. Thefifthpascalrecognizingtextualentailment\\nchallenge. InTAC,2009.\\n[5] S.R.Bowman,G.Angeli,C.Potts,andC.D.Manning. Alargeannotatedcorpusforlearningnatural\\nlanguageinference. EMNLP,2015.\\n[6] D.Cer, M.Diab,E.Agirre,I.Lopez-Gazpio,andL.Specia. Semeval-2017task1: Semantictextual\\nsimilarity-multilingualandcross-lingualfocusedevaluation. arXivpreprintarXiv:1708.00055,2017.\\n[7] S.Chaturvedi,H.Peng,andD.Roth.Storycomprehensionforpredictingwhathappensnext.InProceedings\\nofthe2017ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1603–1614,2017.\\n[8] D.ChenandC.Manning. Afastandaccuratedependencyparserusingneuralnetworks. InProceedings\\nofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages740–750,\\n2014.\\n[9] Z.Chen,H.Zhang,X.Zhang,andL.Zhao. Quoraquestionpairs. https://data.quora.com/First-Quora-\\nDataset-Release-Question-Pairs,2018.\\n[10] R.CollobertandJ.Weston. Aunifiedarchitecturefornaturallanguageprocessing:Deepneuralnetworks\\nwithmultitasklearning. InProceedingsofthe25thinternationalconferenceonMachinelearning,pages\\n160–167.ACM,2008.\\n[11] R.Collobert,J.Weston,L.Bottou,M.Karlen,K.Kavukcuoglu,andP.Kuksa.Naturallanguageprocessing\\n(almost)fromscratch. JournalofMachineLearningResearch,12(Aug):2493–2537,2011.\\n[12] A.Conneau,D.Kiela,H.Schwenk,L.Barrault,andA.Bordes. Supervisedlearningofuniversalsentence\\nrepresentationsfromnaturallanguageinferencedata. EMNLP,2017.\\n[13] A.M.DaiandQ.V.Le.Semi-supervisedsequencelearning.InAdvancesinNeuralInformationProcessing\\nSystems,pages3079–3087,2015.\\n[14] W.B.DolanandC.Brockett.Automaticallyconstructingacorpusofsententialparaphrases.InProceedings\\noftheThirdInternationalWorkshoponParaphrasing(IWP2005),2005.\\n[15] D.Erhan,Y.Bengio,A.Courville,P.-A.Manzagol,P.Vincent,andS.Bengio. Whydoesunsupervised\\npre-traininghelpdeeplearning? JournalofMachineLearningResearch,11(Feb):625–660,2010.\\n[16] S.Gray,A.Radford,andK.P.Diederik. Gpukernelsforblock-sparseweights. 2017.\\n[17] Z.He,S.Liu,M.Li,M.Zhou,L.Zhang,andH.Wang. Learningentityrepresentationforentitydisam-\\nbiguation. InProceedingsofthe51stAnnualMeetingoftheAssociationforComputationalLinguistics\\n(Volume2:ShortPapers),volume2,pages30–34,2013.'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 8, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='[15] D.Erhan,Y.Bengio,A.Courville,P.-A.Manzagol,P.Vincent,andS.Bengio. Whydoesunsupervised\\npre-traininghelpdeeplearning? JournalofMachineLearningResearch,11(Feb):625–660,2010.\\n[16] S.Gray,A.Radford,andK.P.Diederik. Gpukernelsforblock-sparseweights. 2017.\\n[17] Z.He,S.Liu,M.Li,M.Zhou,L.Zhang,andH.Wang. Learningentityrepresentationforentitydisam-\\nbiguation. InProceedingsofthe51stAnnualMeetingoftheAssociationforComputationalLinguistics\\n(Volume2:ShortPapers),volume2,pages30–34,2013.\\n[18] D.HendrycksandK.Gimpel. Bridgingnonlinearitiesandstochasticregularizerswithgaussianerrorlinear\\nunits. arXivpreprintarXiv:1606.08415,2016.\\n[19] K.M.Hermann,T.Kocisky,E.Grefenstette,L.Espeholt,W.Kay,M.Suleyman,andP.Blunsom.Teaching\\nmachinestoreadandcomprehend. InAdvancesinNeuralInformationProcessingSystems,pages1693–\\n1701,2015.\\n[20] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural\\ncomputation,18(7):1527–1554,2006.\\n[21] J.HowardandS.Ruder. Universallanguagemodelfine-tuningfortextclassification. Associationfor\\nComputationalLinguistics(ACL),2018.\\n[22] Y.Jernite, S.R.Bowman, andD.Sontag. Discourse-basedobjectivesforfastunsupervisedsentence\\nrepresentationlearning. arXivpreprintarXiv:1705.00557,2017.\\n[23] Y.JiandJ.Eisenstein. Discriminativeimprovementstodistributionalsentencesimilarity. InProceedings\\nofthe2013ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages891–896,2013.\\n9'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='[24] F.Jiao,S.Wang,C.-H.Lee,R.Greiner,andD.Schuurmans. Semi-supervisedconditionalrandomfields\\nforimprovedsequencesegmentationandlabeling. InProceedingsofthe21stInternationalConferenceon\\nComputationalLinguisticsandthe44thannualmeetingoftheAssociationforComputationalLinguistics,\\npages209–216.AssociationforComputationalLinguistics,2006.\\n[25] T.Khot,A.Sabharwal,andP.Clark. Scitail:Atextualentailmentdatasetfromsciencequestionanswering.\\nInProceedingsofAAAI,2018.\\n[26] Y.Kim. Convolutionalneuralnetworksforsentenceclassification. EMNLP,2014.\\n[27] D.P.KingmaandJ.Ba. Adam:Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980,\\n2014.\\n[28] R.Kiros,Y.Zhu,R.R.Salakhutdinov,R.Zemel,R.Urtasun,A.Torralba,andS.Fidler. Skip-thought\\nvectors. InAdvancesinneuralinformationprocessingsystems,pages3294–3302,2015.\\n[29] N.KitaevandD.Klein. Constituencyparsingwithaself-attentiveencoder. ACL,2018.\\n[30] G.Lai,Q.Xie,H.Liu,Y.Yang,andE.Hovy. Race: Large-scalereadingcomprehensiondatasetfrom\\nexaminations. EMNLP,2017.\\n[31] G.Lample,L.Denoyer,andM.Ranzato. Unsupervisedmachinetranslationusingmonolingualcorpora\\nonly. ICLR,2018.\\n[32] Q.LeandT.Mikolov.Distributedrepresentationsofsentencesanddocuments.InInternationalConference\\nonMachineLearning,pages1188–1196,2014.\\n[33] P.Liang.Semi-supervisedlearningfornaturallanguage.PhDthesis,MassachusettsInstituteofTechnology,\\n2005.\\n[34] P.J.Liu,M.Saleh,E.Pot,B.Goodrich,R.Sepassi,L.Kaiser,andN.Shazeer. Generatingwikipediaby\\nsummarizinglongsequences. ICLR,2018.\\n[35] X.Liu,K.Duh,andJ.Gao. Stochasticanswernetworksfornaturallanguageinference. arXivpreprint\\narXiv:1804.07888,2018.\\n[36] L.LogeswaranandH.Lee. Anefficientframeworkforlearningsentencerepresentations. ICLR,2018.\\n[37] I.LoshchilovandF.Hutter. Fixingweightdecayregularizationinadam. arXivpreprintarXiv:1711.05101,\\n2017.\\n[38] B.McCann,J.Bradbury,C.Xiong,andR.Socher. Learnedintranslation:Contextualizedwordvectors. In\\nAdvancesinNeuralInformationProcessingSystems,pages6297–6308,2017.\\n[39] T.Mikolov, I.Sutskever, K.Chen, G.S.Corrado, andJ.Dean. Distributedrepresentationsofwords\\nandphrasesandtheircompositionality. InAdvancesinneuralinformationprocessingsystems,pages\\n3111–3119,2013.\\n[40] N.Mostafazadeh,M.Roth,A.Louis,N.Chambers,andJ.Allen.Lsdsem2017sharedtask:Thestorycloze\\ntest. InProceedingsofthe2ndWorkshoponLinkingModelsofLexical,SententialandDiscourse-level\\nSemantics,pages46–51,2017.\\n[41] K.Nigam,A.McCallum,andT.Mitchell. Semi-supervisedtextclassificationusingem. Semi-Supervised\\nLearning,pages33–56,2006.'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 9, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='andphrasesandtheircompositionality. InAdvancesinneuralinformationprocessingsystems,pages\\n3111–3119,2013.\\n[40] N.Mostafazadeh,M.Roth,A.Louis,N.Chambers,andJ.Allen.Lsdsem2017sharedtask:Thestorycloze\\ntest. InProceedingsofthe2ndWorkshoponLinkingModelsofLexical,SententialandDiscourse-level\\nSemantics,pages46–51,2017.\\n[41] K.Nigam,A.McCallum,andT.Mitchell. Semi-supervisedtextclassificationusingem. Semi-Supervised\\nLearning,pages33–56,2006.\\n[42] J.Pennington,R.Socher,andC.Manning. Glove:Globalvectorsforwordrepresentation. InProceedings\\nofthe2014conferenceonempiricalmethodsinnaturallanguageprocessing(EMNLP),pages1532–1543,\\n2014.\\n[43] M.E.Peters,W.Ammar,C.Bhagavatula,andR.Power. Semi-supervisedsequencetaggingwithbidirec-\\ntionallanguagemodels. ACL,2017.\\n[44] M.E.Peters,M.Neumann,M.Iyyer,M.Gardner,C.Clark,K.Lee,andL.Zettlemoyer. Deepcontextual-\\nizedwordrepresentations. NAACL,2018.\\n[45] Y.Qi,D.S.Sachan,M.Felix,S.J.Padmanabhan,andG.Neubig. Whenandwhyarepre-trainedword\\nembeddingsusefulforneuralmachinetranslation? NAACL,2018.\\n10'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 10, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='[46] A.RahmanandV.Ng. Resolvingcomplexcasesofdefinitepronouns:thewinogradschemachallenge. In\\nProceedingsofthe2012JointConferenceonEmpiricalMethodsinNaturalLanguageProcessingand\\nComputationalNaturalLanguageLearning,pages777–789.AssociationforComputationalLinguistics,\\n2012.\\n[47] P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang. Squad:100,000+questionsformachinecomprehension\\noftext. EMNLP,2016.\\n[48] P.Ramachandran,P.J.Liu,andQ.V.Le. Unsupervisedpretrainingforsequencetosequencelearning.\\narXivpreprintarXiv:1611.02683,2016.\\n[49] M.Ranzato,C.Poultney,S.Chopra,andY.LeCun. Efficientlearningofsparserepresentationswithan\\nenergy-basedmodel. InAdvancesinneuralinformationprocessingsystems,pages1137–1144,2007.\\n[50] M.Rei. Semi-supervisedmultitasklearningforsequencelabeling. ACL,2017.\\n[51] H.RobbinsandS.Monro. Astochasticapproximationmethod. Theannalsofmathematicalstatistics,\\npages400–407,1951.\\n[52] T.Rocktäschel,E.Grefenstette,K.M.Hermann,T.Kocˇisky`,andP.Blunsom. Reasoningaboutentailment\\nwithneuralattention. arXivpreprintarXiv:1509.06664,2015.\\n[53] R.Sennrich,B.Haddow,andA.Birch.Neuralmachinetranslationofrarewordswithsubwordunits.arXiv\\npreprintarXiv:1508.07909,2015.\\n[54] R.Socher,A.Perelygin,J.Wu,J.Chuang,C.D.Manning,A.Ng,andC.Potts. Recursivedeepmodelsfor\\nsemanticcompositionalityoverasentimenttreebank. InProceedingsofthe2013conferenceonempirical\\nmethodsinnaturallanguageprocessing,pages1631–1642,2013.\\n[55] S.Srinivasan,R.Arora,andM.Riedl. Asimpleandeffectiveapproachtothestoryclozetest. arXiv\\npreprintarXiv:1803.05547,2018.\\n[56] S.Subramanian,A.Trischler,Y.Bengio,andC.J.Pal. Learninggeneralpurposedistributedsentence\\nrepresentationsvialargescalemulti-tasklearning. arXivpreprintarXiv:1804.00079,2018.\\n[57] J.SuzukiandH.Isozaki. Semi-supervisedsequentiallabelingandsegmentationusinggiga-wordscale\\nunlabeleddata. ProceedingsofACL-08:HLT,pages665–673,2008.\\n[58] Y.Tay,L.A.Tuan,andS.C.Hui. Acompare-propagatearchitecturewithalignmentfactorizationfor\\nnaturallanguageinference. arXivpreprintarXiv:1801.00102,2017.\\n[59] Y.Tay,L.A.Tuan,andS.C.Hui. Multi-rangereasoningformachinecomprehension. arXivpreprint\\narXiv:1803.09074,2018.\\n[60] J.Tian,Z.Zhou,M.Lan,andY.Wu. Ecnuatsemeval-2017task1:Leveragekernel-basedtraditionalnlp\\nfeaturesandneuralnetworkstobuildauniversalmodelformultilingualandcross-lingualsemantictextual\\nsimilarity. InProceedingsofthe11thInternationalWorkshoponSemanticEvaluation(SemEval-2017),\\npages191–197,2017.\\n[61] Y.Tsvetkov. Opportunitiesandchallengesinworkingwithlow-resourcelanguages. CMU,2017.'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 10, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='[59] Y.Tay,L.A.Tuan,andS.C.Hui. Multi-rangereasoningformachinecomprehension. arXivpreprint\\narXiv:1803.09074,2018.\\n[60] J.Tian,Z.Zhou,M.Lan,andY.Wu. Ecnuatsemeval-2017task1:Leveragekernel-basedtraditionalnlp\\nfeaturesandneuralnetworkstobuildauniversalmodelformultilingualandcross-lingualsemantictextual\\nsimilarity. InProceedingsofthe11thInternationalWorkshoponSemanticEvaluation(SemEval-2017),\\npages191–197,2017.\\n[61] Y.Tsvetkov. Opportunitiesandchallengesinworkingwithlow-resourcelanguages. CMU,2017.\\n[62] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Ł.Kaiser,andI.Polosukhin.\\nAttentionisallyouneed. InAdvancesinNeuralInformationProcessingSystems,pages6000–6010,2017.\\n[63] P.Vincent,H.Larochelle,Y.Bengio,andP.-A.Manzagol. Extractingandcomposingrobustfeatureswith\\ndenoisingautoencoders. InProceedingsofthe25thinternationalconferenceonMachinelearning,pages\\n1096–1103.ACM,2008.\\n[64] A.Wang,A.Singh,J.Michael,F.Hill,O.Levy,andS.R.Bowman. Glue:Amulti-taskbenchmarkand\\nanalysisplatformfornaturallanguageunderstanding. arXivpreprintarXiv:1804.07461,2018.\\n[65] A.Warstadt,A.Singh,andS.R.Bowman. Corpusoflinguisticacceptability. http://nyu-mll.github.io/cola,\\n2018.\\n[66] A.Williams,N.Nangia,andS.R.Bowman.Abroad-coveragechallengecorpusforsentenceunderstanding\\nthroughinference. NAACL,2018.\\n[67] Y. Xu, J. Liu, J. Gao, Y. Shen, and X. Liu. Towards human-level machine reading comprehension:\\nReasoningandinferencewithmultiplestrategies. arXivpreprintarXiv:1711.04964,2017.\\n11'),\n Document(metadata={'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 11, 'total_pages': 12, 'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False'}, page_content='[68] D.Yu,L.Deng,andG.Dahl. Rolesofpre-trainingandfine-tuningincontext-dependentdbn-hmmsfor\\nreal-worldspeechrecognition. InProc.NIPSWorkshoponDeepLearningandUnsupervisedFeature\\nLearning,2010.\\n[69] R.Zhang,P.Isola,andA.A.Efros. Split-brainautoencoders: Unsupervisedlearningbycross-channel\\nprediction. InCVPR,volume1,page6,2017.\\n[70] X.Zhu. Semi-supervisedlearningliteraturesurvey. 2005.\\n[71] Y.Zhu,R.Kiros,R.Zemel,R.Salakhutdinov,R.Urtasun,A.Torralba,andS.Fidler. Aligningbooksand\\nmovies:Towardsstory-likevisualexplanationsbywatchingmoviesandreadingbooks. InProceedingsof\\ntheIEEEinternationalconferenceoncomputervision,pages19–27,2015.\\n12')]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "loader = PDFPlumberLoader(\"/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size = 1000, chunk_overlap = 200)\n",
    "docs  = text_splitter.split_documents(documents)\n",
    "docs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-15T09:52:16.492487Z",
     "start_time": "2025-01-15T09:52:15.351016Z"
    }
   },
   "id": "cccc3b0d18a8a4de",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc587349b5ce01b3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-huggingface"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T10:40:54.584638Z",
     "start_time": "2025-01-14T10:40:53.188122Z"
    }
   },
   "id": "d914e1cbe65fe2fe",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vaibhav/PycharmProjects/llm-visplain/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-mpnet-base-v2\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-15T09:52:29.747329Z",
     "start_time": "2025-01-15T09:52:24.475775Z"
    }
   },
   "id": "756bac1208e55518",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vStore = Chroma.from_documents(documents=docs, embedding=embedding)\n",
    "\n",
    "retriever = vStore.as_retriever()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-15T09:52:32.371212Z",
     "start_time": "2025-01-15T09:52:31.587982Z"
    }
   },
   "id": "e15eaca8b81c60f8",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Construct chain"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f161df8008c997f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "promptTemplate = PromptTemplate(\n",
    "    input_variables=[\"context\"], \n",
    "    template=\"\"\"\n",
    "    You are a document analyzer and very good at getting LLM architecture details from the research papers. If you are unable to find information about a particular component or parameter in the architecture, provide the architecture details as you find them. \n",
    "    This should be the flow of your answer:\n",
    "    {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "documentChain = create_stuff_documents_chain(llm, promptTemplate)\n",
    "retrievalChain = create_retrieval_chain(retriever, documentChain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-15T09:52:35.148296Z",
     "start_time": "2025-01-15T09:52:35.142126Z"
    }
   },
   "id": "2ba886c58916120d",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': 'What are the architectural components of GPT 1?',\n 'context': [Document(metadata={'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 6, 'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'total_pages': 12}, page_content='Table4: Semanticsimilarityandclassificationresults,comparingourmodelwithcurrentstate-of-the-\\nartmethods. AlltaskevaluationsinthistableweredoneusingtheGLUEbenchmark. (mc=Mathews\\ncorrelation,acc=Accuracy,pc=Pearsoncorrelation)\\nMethod Classification SemanticSimilarity GLUE\\nCoLA SST2 MRPC STSB QQP\\n(mc) (acc) (F1) (pc) (F1)\\nSparsebytemLSTM[16] - 93.2 - - - -\\nTF-KLD[23] - - 86.0 - - -\\nECNU(mixedensemble)[60] - - - 81.0 - -\\nSingle-taskBiLSTM+ELMo+Attn[64] 35.0 90.2 80.2 55.5 66.1 64.8\\nMulti-taskBiLSTM+ELMo+Attn[64] 18.9 91.6 83.5 72.8 63.3 68.9\\nFinetunedTransformerLM(ours) 45.4 91.3 82.3 82.0 70.3 72.8\\nOverall,ourapproachachievesnewstate-of-the-artresultsin9outofthe12datasetsweevaluate\\non,outperformingensemblesinmanycases. Ourresultsalsoindicatethatourapproachworkswell\\nacrossdatasetsofdifferentsizes,fromsmallerdatasetssuchasSTS-B(≈5.7ktrainingexamples)–\\ntothelargestone–SNLI(≈550ktrainingexamples).\\n5 Analysis\\nImpactofnumberoflayerstransferred Weobservedtheimpactoftransferringavariablenumber\\noflayersfromunsupervisedpre-trainingtothesupervisedtargettask. Figure2(left)illustratesthe\\nperformanceofourapproachonMultiNLIandRACEasafunctionofthenumberoflayerstransferred.\\nWeobservethestandardresultthattransferringembeddingsimprovesperformanceandthateach\\ntransformerlayerprovidesfurtherbenefitsupto9%forfulltransferonMultiNLI.Thisindicatesthat\\neachlayerinthepre-trainedmodelcontainsusefulfunctionalityforsolvingtargettasks.\\nFigure 2: (left) Effect of transferring increasing number of layers from the pre-trained language\\nmodel on RACE and MultiNLI. (right) Plot showing the evolution of zero-shot performance on\\ndifferenttasksasafunctionofLMpre-trainingupdates. Performancepertaskisnormalizedbetween\\narandomguessbaselineandthecurrentstate-of-the-artwithasinglemodel.\\nZero-shotBehaviors We’dliketobetterunderstandwhylanguagemodelpre-trainingoftransform-\\nersiseffective. Ahypothesisisthattheunderlyinggenerativemodellearnstoperformmanyofthe\\ntasksweevaluateoninordertoimproveitslanguagemodelingcapabilityandthatthemorestructured\\n7'),\n  Document(metadata={'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 6, 'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'total_pages': 12}, page_content='Table4: Semanticsimilarityandclassificationresults,comparingourmodelwithcurrentstate-of-the-\\nartmethods. AlltaskevaluationsinthistableweredoneusingtheGLUEbenchmark. (mc=Mathews\\ncorrelation,acc=Accuracy,pc=Pearsoncorrelation)\\nMethod Classification SemanticSimilarity GLUE\\nCoLA SST2 MRPC STSB QQP\\n(mc) (acc) (F1) (pc) (F1)\\nSparsebytemLSTM[16] - 93.2 - - - -\\nTF-KLD[23] - - 86.0 - - -\\nECNU(mixedensemble)[60] - - - 81.0 - -\\nSingle-taskBiLSTM+ELMo+Attn[64] 35.0 90.2 80.2 55.5 66.1 64.8\\nMulti-taskBiLSTM+ELMo+Attn[64] 18.9 91.6 83.5 72.8 63.3 68.9\\nFinetunedTransformerLM(ours) 45.4 91.3 82.3 82.0 70.3 72.8\\nOverall,ourapproachachievesnewstate-of-the-artresultsin9outofthe12datasetsweevaluate\\non,outperformingensemblesinmanycases. Ourresultsalsoindicatethatourapproachworkswell\\nacrossdatasetsofdifferentsizes,fromsmallerdatasetssuchasSTS-B(≈5.7ktrainingexamples)–\\ntothelargestone–SNLI(≈550ktrainingexamples).\\n5 Analysis\\nImpactofnumberoflayerstransferred Weobservedtheimpactoftransferringavariablenumber\\noflayersfromunsupervisedpre-trainingtothesupervisedtargettask. Figure2(left)illustratesthe\\nperformanceofourapproachonMultiNLIandRACEasafunctionofthenumberoflayerstransferred.\\nWeobservethestandardresultthattransferringembeddingsimprovesperformanceandthateach\\ntransformerlayerprovidesfurtherbenefitsupto9%forfulltransferonMultiNLI.Thisindicatesthat\\neachlayerinthepre-trainedmodelcontainsusefulfunctionalityforsolvingtargettasks.\\nFigure 2: (left) Effect of transferring increasing number of layers from the pre-trained language\\nmodel on RACE and MultiNLI. (right) Plot showing the evolution of zero-shot performance on\\ndifferenttasksasafunctionofLMpre-trainingupdates. Performancepertaskisnormalizedbetween\\narandomguessbaselineandthecurrentstate-of-the-artwithasinglemodel.\\nZero-shotBehaviors We’dliketobetterunderstandwhylanguagemodelpre-trainingoftransform-\\nersiseffective. Ahypothesisisthattheunderlyinggenerativemodellearnstoperformmanyofthe\\ntasksweevaluateoninordertoimproveitslanguagemodelingcapabilityandthatthemorestructured\\n7'),\n  Document(metadata={'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 7, 'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'total_pages': 12}, page_content='Table5: Analysisofvariousmodelablationsondifferenttasks. Avg. scoreisaunweightedaverage\\nofalltheresults. (mc=Mathewscorrelation,acc=Accuracy,pc=Pearsoncorrelation)\\nMethod Avg.Score CoLA SST2 MRPC STSB QQP MNLI QNLI RTE\\n(mc) (acc) (F1) (pc) (F1) (acc) (acc) (acc)\\nTransformerw/auxLM(full) 74.7 45.4 91.3 82.3 82.0 70.3 81.8 88.1 56.0\\nTransformerw/opre-training 59.9 18.9 84.0 79.4 30.9 65.5 75.7 71.2 53.8\\nTransformerw/oauxLM 75.0 47.9 92.0 84.9 83.2 69.8 81.1 86.9 54.4\\nLSTMw/auxLM 69.1 30.3 90.5 83.2 71.8 68.1 73.7 81.1 54.6\\nattentionalmemoryofthetransformerassistsintransfercomparedtoLSTMs. Wedesignedaseries\\nofheuristicsolutionsthatusetheunderlyinggenerativemodeltoperformtaskswithoutsupervised\\nfinetuning. Wevisualizetheeffectivenessoftheseheuristicsolutionsoverthecourseofgenerative\\npre-traininginFig2(right). Weobservetheperformanceoftheseheuristicsisstableandsteadily\\nincreasesovertrainingsuggestingthatgenerativepretrainingsupportsthelearningofawidevariety\\noftaskrelevantfunctionality. WealsoobservetheLSTMexhibitshighervarianceinitszero-shot\\nperformancesuggestingthattheinductivebiasoftheTransformerarchitectureassistsintransfer.\\nForCoLA(linguisticacceptability),examplesarescoredastheaveragetokenlog-probabilitythe\\ngenerativemodelassignsandpredictionsaremadebythresholding. ForSST-2(sentimentanalysis),\\nweappendthetokenverytoeachexampleandrestrictthelanguagemodel’soutputdistributiontoonly\\nthewordspositiveandnegativeandguessthetokenitassignshigherprobabilitytoastheprediction.\\nForRACE(questionanswering),wepicktheanswerthegenerativemodelassignsthehighestaverage\\ntokenlog-probabilitywhenconditionedonthedocumentandquestion. ForDPRD[46](winograd\\nschemas),wereplacethedefinitepronounwiththetwopossiblereferrentsandpredicttheresolution\\nthatthegenerativemodelassignshigheraveragetokenlog-probabilitytotherestofthesequence\\nafterthesubstitution.\\nAblation studies We perform three different ablation studies (Table 5). First, we examine the\\nperformanceofourmethodwithouttheauxiliaryLMobjectiveduringfine-tuning. Weobservethat\\ntheauxiliaryobjectivehelpsontheNLItasksandQQP.Overall,thetrendsuggeststhatlargerdatasets\\nbenefitfromtheauxiliaryobjectivebutsmallerdatasetsdonot. Second,weanalyzetheeffectofthe\\nTransformerbycomparingitwithasinglelayer2048unitLSTMusingthesameframework. We\\nobservea5.6averagescoredropwhenusingtheLSTMinsteadoftheTransformer. TheLSTMonly\\noutperformstheTransformerononedataset–MRPC.Finally,wealsocomparewithourtransformer\\narchitecturedirectlytrainedonsupervisedtargettasks,withoutpre-training. Weobservethatthelack\\nofpre-traininghurtsperformanceacrossallthetasks,resultingina14.8%decreasecomparedtoour\\nfullmodel.\\n6 Conclusion\\nWe introduced a framework for achieving strong natural language understanding with a single\\ntask-agnosticmodelthroughgenerativepre-traininganddiscriminativefine-tuning. Bypre-training\\non a diverse corpus with long stretches of contiguous text our model acquires significant world\\nknowledgeandabilitytoprocesslong-rangedependencieswhicharethensuccessfullytransferredto\\nsolvingdiscriminativetaskssuchasquestionanswering,semanticsimilarityassessment,entailment'),\n  Document(metadata={'Author': '', 'CreationDate': 'D:20180608191434Z', 'Creator': 'LaTeX with hyperref package', 'Keywords': '', 'ModDate': 'D:20180608191434Z', 'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.18 (TeX Live 2017) kpathsea version 6.2.3', 'Producer': 'pdfTeX-1.40.18', 'Subject': '', 'Title': '', 'Trapped': 'False', 'file_path': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'page': 7, 'source': '/Users/vaibhav/PycharmProjects/llm-visplain/language_understanding_paper.pdf', 'total_pages': 12}, page_content='Table5: Analysisofvariousmodelablationsondifferenttasks. Avg. scoreisaunweightedaverage\\nofalltheresults. (mc=Mathewscorrelation,acc=Accuracy,pc=Pearsoncorrelation)\\nMethod Avg.Score CoLA SST2 MRPC STSB QQP MNLI QNLI RTE\\n(mc) (acc) (F1) (pc) (F1) (acc) (acc) (acc)\\nTransformerw/auxLM(full) 74.7 45.4 91.3 82.3 82.0 70.3 81.8 88.1 56.0\\nTransformerw/opre-training 59.9 18.9 84.0 79.4 30.9 65.5 75.7 71.2 53.8\\nTransformerw/oauxLM 75.0 47.9 92.0 84.9 83.2 69.8 81.1 86.9 54.4\\nLSTMw/auxLM 69.1 30.3 90.5 83.2 71.8 68.1 73.7 81.1 54.6\\nattentionalmemoryofthetransformerassistsintransfercomparedtoLSTMs. Wedesignedaseries\\nofheuristicsolutionsthatusetheunderlyinggenerativemodeltoperformtaskswithoutsupervised\\nfinetuning. Wevisualizetheeffectivenessoftheseheuristicsolutionsoverthecourseofgenerative\\npre-traininginFig2(right). Weobservetheperformanceoftheseheuristicsisstableandsteadily\\nincreasesovertrainingsuggestingthatgenerativepretrainingsupportsthelearningofawidevariety\\noftaskrelevantfunctionality. WealsoobservetheLSTMexhibitshighervarianceinitszero-shot\\nperformancesuggestingthattheinductivebiasoftheTransformerarchitectureassistsintransfer.\\nForCoLA(linguisticacceptability),examplesarescoredastheaveragetokenlog-probabilitythe\\ngenerativemodelassignsandpredictionsaremadebythresholding. ForSST-2(sentimentanalysis),\\nweappendthetokenverytoeachexampleandrestrictthelanguagemodel’soutputdistributiontoonly\\nthewordspositiveandnegativeandguessthetokenitassignshigherprobabilitytoastheprediction.\\nForRACE(questionanswering),wepicktheanswerthegenerativemodelassignsthehighestaverage\\ntokenlog-probabilitywhenconditionedonthedocumentandquestion. ForDPRD[46](winograd\\nschemas),wereplacethedefinitepronounwiththetwopossiblereferrentsandpredicttheresolution\\nthatthegenerativemodelassignshigheraveragetokenlog-probabilitytotherestofthesequence\\nafterthesubstitution.\\nAblation studies We perform three different ablation studies (Table 5). First, we examine the\\nperformanceofourmethodwithouttheauxiliaryLMobjectiveduringfine-tuning. Weobservethat\\ntheauxiliaryobjectivehelpsontheNLItasksandQQP.Overall,thetrendsuggeststhatlargerdatasets\\nbenefitfromtheauxiliaryobjectivebutsmallerdatasetsdonot. Second,weanalyzetheeffectofthe\\nTransformerbycomparingitwithasinglelayer2048unitLSTMusingthesameframework. We\\nobservea5.6averagescoredropwhenusingtheLSTMinsteadoftheTransformer. TheLSTMonly\\noutperformstheTransformerononedataset–MRPC.Finally,wealsocomparewithourtransformer\\narchitecturedirectlytrainedonsupervisedtargettasks,withoutpre-training. Weobservethatthelack\\nofpre-traininghurtsperformanceacrossallthetasks,resultingina14.8%decreasecomparedtoour\\nfullmodel.\\n6 Conclusion\\nWe introduced a framework for achieving strong natural language understanding with a single\\ntask-agnosticmodelthroughgenerativepre-traininganddiscriminativefine-tuning. Bypre-training\\non a diverse corpus with long stretches of contiguous text our model acquires significant world\\nknowledgeandabilitytoprocesslong-rangedependencieswhicharethensuccessfullytransferredto\\nsolvingdiscriminativetaskssuchasquestionanswering,semanticsimilarityassessment,entailment')],\n 'answer': 'I\\'d be happy to help you extract the LLM architecture details from this research paper!\\n\\nFrom the provided text, I can identify the following key points:\\n\\n1. **Pre-training:** The model is pre-trained on a diverse corpus with long stretches of contiguous text.\\n2. **Generative pre-training:** The pre-training process involves generative models that learn to predict the next word in a sequence.\\n3. **Discriminative fine-tuning:** After pre-training, the model is fine-tuned for discriminative tasks using supervised learning.\\n4. **Task-agnostic architecture:** The model\\'s architecture is designed to be task-agnostic, allowing it to adapt to different downstream tasks without significant modifications.\\n\\nThere is no specific information provided about the LLM architecture (e.g., transformer-based, recurrent neural network (RNN), or convolutional neural network (CNN)), but based on the context and the fact that the paper discusses a \"Transformer\" model in one of the tables, I assume that the architecture is likely a transformer-based model.\\n\\nPlease let me know if you\\'d like me to help with anything else!'}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrievalChain.invoke({\"input\": \"What are the architectural components of GPT 1?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-15T09:53:00.213298Z",
     "start_time": "2025-01-15T09:52:37.825139Z"
    }
   },
   "id": "d9a996f108d240e8",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "795d0ad90ffe6387"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
