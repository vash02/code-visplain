import numpy as np
import ollama
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


class KAGHandler:
    def __init__(self, embedding_generator, model = "micorsoft/codebert-base", llama_model_name='llama3'):
        self.embedding_generator = embedding_generator
        self.llama_model_name = llama_model_name
    def query_kag(self, query, k=3, G=None, embeddings=None, code_chunks=None):
        """
        Perform Knowledge-Augmented Generation (KAG) based on the code components in the graph.
        This function uses CodeBERT for embeddings and LLaMA for generating answers based on the context.

        Args:
            query (str): The query to ask.
            k (int): The number of top similar nodes to consider for context generation.
            G (networkx.Graph): The graph that represents the code components.
            embeddings (dict): Pre-computed embeddings for the code nodes.
            code_chunks (dict): The actual code snippets from the repository.

        Returns:
            str: Full prompt for LLaMA to generate an answer based on the context.
        """
        # Step 1: Generate embedding for the query using CodeBERT
        tokenizer = self.embedding_generator.tokenizer
        model = self.embedding_generator.model
        inputs = tokenizer(query, return_tensors="pt", truncation=True, padding=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)
            query_embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy().squeeze()

        # Step 2: Find the node with the highest similarity (query vs. node embeddings)
        similarities = []
        for node in G.nodes:
            node_embedding = G.nodes[node]["embedding"]
            similarity = np.dot(query_embedding, node_embedding) / (
                        np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding))  # Cosine similarity
            similarities.append((node, similarity))

        # Step 3: Sort by similarity and get top k
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_k_nodes = [node for node, _ in similarities[:k]]

        # Debug: Check if top_k_nodes are correct
        print(f"Top {k} nodes: {top_k_nodes}")

        # Step 4: Retrieve the corresponding code chunks (files) from the graph nodes
        context = []
        for node in top_k_nodes:
            file_name = G.nodes[node]['file_name']
            chunk = code_chunks[node][1]  # Assuming code_chunks[node][1] contains the code snippet

            # Debug: Check the content of each chunk
            print(f"Adding context from file: {file_name}")
            print(f"Code chunk: {chunk[:100]}...")  # Print first 100 characters of the chunk for debugging

            context.append(f"File: {file_name}\nChunk:\n{chunk}\n")

        # Step 5: Combine the query with the context for the LLaMA model
        full_prompt = f"Query: {query}\nContext:\n" + "\n".join(context)

        # Debug: Print the full prompt before sending it to LLaMA
        print(f"Full prompt:\n{full_prompt[:500]}...")  # Print first 500 characters for debugging

        # Return the full prompt for LLaMA to generate an answer
        return full_prompt

    def generate_answer_with_ollama(self, context):
        """
        Generate an answer using the Ollama API for the provided context.

        Args:
            context (str): The context to be used by LLaMA (via Ollama) for answering the query.

        Returns:
            str: The answer generated by Ollama's LLaMA model.
        """
        # Send the prompt to Ollama for LLaMA processing
        response = ollama.chat(model=self.llama_model_name, messages=[{"role": "user", "content": context}])

        # Debug: Check the response structure
        # print(f"Response from Ollama: {response}")

        # Extract the answer
        answer = response["message"]["content"]
        return answer
